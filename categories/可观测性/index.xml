<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>可观测性 on fmt.Println(&#34;Li Duo&#34;)</title>
        <link>https://lizonglingo.github.io/categories/%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/</link>
        <description>Recent content in 可观测性 on fmt.Println(&#34;Li Duo&#34;)</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-CN</language>
        <lastBuildDate>Sun, 15 May 2022 21:07:08 +0800</lastBuildDate><atom:link href="https://lizonglingo.github.io/categories/%E5%8F%AF%E8%A7%82%E6%B5%8B%E6%80%A7/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Cocktail: A Multidimensional Optimization for Model Serving in Cloud</title>
        <link>https://lizonglingo.github.io/p/cocktail-a-multidimensional-optimization-for-model-serving-in-cloud/</link>
        <pubDate>Sun, 15 May 2022 21:07:08 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/cocktail-a-multidimensional-optimization-for-model-serving-in-cloud/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;来源：&lt;a class=&#34;link&#34; href=&#34;https://www.usenix.org/conference/nsdi22/presentation/gunasekaran&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;NSDI&#39;22&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;推荐阅读！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;h3 id=&#34;背景&#34;&gt;背景&lt;/h3&gt;
&lt;p&gt;越来越多的ML模型运行在公有云环境下。&lt;strong&gt;为这些模型服务的框架能够以最小的延迟提供高度准确的预测，并降低部署成本，这一点至关重要。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;关键点&#34;&gt;关键点&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;模型集成可以通过智能地将不同模型并行组合来解决精度差距问题。然而，在运行时动态地选择合适的模型，以以最小的部署成本、低延迟来满足预期的准确性。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;本文工作&#34;&gt;本文工作&lt;/h3&gt;
&lt;p&gt;提出&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;，基于模型集成的成本效益模型服务框架。包含两个关键的组件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个&lt;strong&gt;动态模型选择框架&lt;/strong&gt;，在满足&lt;strong&gt;精度和延迟&lt;/strong&gt;要求的同时，&lt;strong&gt;减少了集成中的模型数量&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;一个采用&lt;strong&gt;分布式主动自动伸缩策略的自适应资源管理&lt;/strong&gt;（RM，Resource Management）框架，有效地为模型分配资源。RM框架利用瞬态虚拟机实例来降低公共云中的部署成本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时在AWS EC2实例中实现了一个原型系统，演示了使用各种工作负载的详尽评估。结果显示Cocktail减少了部署花费1.45x，与最先进的模型服务框架相比，减少了2x延迟，并满足高达96%的请求的目标精度。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;h3 id=&#34;背景及问题引出&#34;&gt;背景及问题引出&lt;/h3&gt;
&lt;p&gt;背景案例：Facebook为用户交互应用程序提供了数万亿的推理请求，如对新提要进行排名，对照片进行分类等。这些应用程序必须在&lt;strong&gt;亚毫秒延迟&lt;/strong&gt;[27、34、35、39、44、83]提供准确的预测，因为它们严重影响用户体验。&lt;/p&gt;
&lt;p&gt;随着许多应用使用ML技术增强其用户体验，这种趋势正在扩大。&lt;/p&gt;
&lt;p&gt;通常这种模型服务运行在云平台上，如一些&lt;em&gt;model-serving&lt;/em&gt;框架[6, 28, 60]。&lt;/p&gt;
&lt;h3 id=&#34;挑战&#34;&gt;挑战&lt;/h3&gt;
&lt;p&gt;由于训练数据以及计算和内存资源紧张[59,65,84]造成的高方差一直是设计高精度和低延迟模型的主要障碍&lt;/p&gt;
&lt;p&gt;不同于单模型推理任务，&lt;code&gt;ensemble learning&lt;/code&gt;集成学习可以进一步提高服务精确度。（如，多模型的图片分类任务会提高最终的精确度）&lt;/p&gt;
&lt;p&gt;然而，对于集成，由于&lt;strong&gt;每个请求都需要运行大量的模型[27,56]而导致的非常高的资源占用，加剧了公共云的部署成本，并导致延迟的高度变化&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;因此，本文解决的主要问题为：&lt;/p&gt;
&lt;p&gt;⭐集成单一的模型推理服务；&lt;/p&gt;
&lt;p&gt;⭐同时提高模型服务的准确度；&lt;/p&gt;
&lt;p&gt;⭐并最小化部署成本。&lt;/p&gt;
&lt;h3 id=&#34;现有技术的不足&#34;&gt;现有技术的不足&lt;/h3&gt;
&lt;p&gt;对最先进的集成模型服务框架进行分析，存在如下不足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在像Clipper[27]这样的框架中使用的&lt;strong&gt;集成模型选择策略是静态的，因为它们集成了所有可用的模型，并只专注于最小化准确性损失。这将导致更高的延迟，并进一步扩大资源使用&lt;/strong&gt;，从而加重部署成本。&lt;/li&gt;
&lt;li&gt;现有的集合权重估计[87]&lt;strong&gt;计算复杂度高&lt;/strong&gt;，在实践中仅限于一小部分现成模型。这导致&lt;strong&gt;精度损失严重&lt;/strong&gt;。此外，采用线性集成技术(如模型平均)计算量大[80]，且对大量可用模型&lt;strong&gt;不可伸缩，缺少弹性&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;现有的集成系统&lt;strong&gt;不关注公共云基础设施中的模型部署&lt;/strong&gt;，没有注意到部署成本和延迟。&lt;/li&gt;
&lt;li&gt;对单一模型的&lt;strong&gt;资源管理模采用的策略不能直接扩展到集成系统中&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，重复之前亟待解决的问题：&lt;/p&gt;
&lt;p&gt;⚠️&lt;strong&gt;如何解决集成框架的成本、精度和延迟等复杂优化问题？&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;本文工作-1&#34;&gt;本文工作&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;，首个&lt;strong&gt;成本友好&lt;/strong&gt;、&lt;strong&gt;集成多模型&lt;/strong&gt;的ML服务框架，&lt;strong&gt;针对于分类推理任务&lt;/strong&gt;，有很好的&lt;strong&gt;精确度和低延迟&lt;/strong&gt;表现。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;它使用下面三方面解决框架优化问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出了一种动态模型选择策略，在满足延迟和精度要求的同时，显著减少了集成中使用的模型数量；&lt;/li&gt;
&lt;li&gt;利用分布式自动伸缩策略来减少托管集成模型的延迟可变性和资源消耗；&lt;/li&gt;
&lt;li&gt;利用transient VMs技术减少了推理服务部署成本（比传统的虚拟机减少79%-90%的成本）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;contributions&#34;&gt;Contributions&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;通过&lt;strong&gt;描述集成模型的精度与延迟&lt;/strong&gt;，我们发现在给定的延迟下&lt;strong&gt;谨慎地选择可用模型的子集可以达到目标精度&lt;/strong&gt;。在&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;中利用这一点，&lt;strong&gt;设计了一种新颖的动态模型选择策略，在保证准确性的同时大大减少了模型的数量&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关注基于分类的推理&lt;/strong&gt;，&lt;strong&gt;最小化来自多个模型的预测偏差&lt;/strong&gt;。&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;采用了一个pre-class加权多数投票政策，这使得它具有&lt;strong&gt;可扩展性&lt;/strong&gt;，与传统加权平均相比，有效地打破了不同模型之间的联系，从而最大限度地提高了准确性。&lt;/li&gt;
&lt;li&gt;集成模型资源需求的变动会导致资源的过度供应，为了最小化资源，我们构建了一个&lt;strong&gt;分布式的加权自动伸缩策略&lt;/strong&gt;，该策略利用重要&lt;strong&gt;抽样技术主动地为每个模型分配资源&lt;/strong&gt;。&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;使用transient VMs降低模型在云平台上部署的成本。&lt;/li&gt;
&lt;li&gt;使用AWS EC2的CPU和GPU实例，实现了原型系统&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;并对不同的请求进行了评估。与最先进的模型服务系统相比，部署成本降低1.4x，精确度提升至96%，延迟减少2x。&lt;/li&gt;
&lt;li&gt;同时表明，集成模型的&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;，&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;可以通过将准确度损失限制在0.6%以内来适应实例故障，对故障容忍性有较大提升。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;background-and-motivation&#34;&gt;Background and Motivation&lt;/h2&gt;
&lt;p&gt;本章结构如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;分析现有公有云中的集成模型服务；&lt;/li&gt;
&lt;li&gt;指出这些服务存在的问题；&lt;/li&gt;
&lt;li&gt;表明&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;基于以上问题需要做的改进。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;model-serving-in-public-cloud&#34;&gt;Model Serving in Public Cloud&lt;/h3&gt;
&lt;p&gt;现有公有云模型服务架构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515153406902.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515153406902&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;application层&#34;&gt;Application层&lt;/h4&gt;
&lt;p&gt;关注SLO，本文指End2End的响应时间。如Ads服务在100ms、推荐服务可以容忍1000ms。&lt;/p&gt;
&lt;h4 id=&#34;model-层和-framework-层&#34;&gt;Model 层和 Framework 层&lt;/h4&gt;
&lt;p&gt;部署的如TensorFlow、PyTorch框架。以及提供的不同模型（这里以分类模型为例）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515153920197.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515153920197&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;根据应用程序类型，最大的模型集成尺寸可以从数十到数百个模型不等。&lt;/p&gt;
&lt;h4 id=&#34;cloud-层&#34;&gt;Cloud 层&lt;/h4&gt;
&lt;p&gt;以VMs或者Container提供资源隔离和运行环境，基于异构的CPU、GPU实例。&lt;/p&gt;
&lt;p&gt;其中，&lt;strong&gt;瞬态实例&lt;/strong&gt;[69]与传统的VM类似，但可以由云提供商在任何时间通过中断通知撤销。这些资源的供应延迟、实例持久性和模型打包花费直接影响到托管模型服务的延迟和成本。&lt;/p&gt;
&lt;p&gt;本文&lt;strong&gt;从模型选择的角度关注于提高准确性和延迟，并从成本的角度考虑实例类型&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;related-work&#34;&gt;Related Work&lt;/h3&gt;
&lt;p&gt;下图为本文工作和先前相关工作的对比：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515172130602.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515172130602&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;1️⃣现有的集成模型案例：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Azure ML-studio：最初集成了5个模型，现在逐渐扩展到200个模型。&lt;/li&gt;
&lt;li&gt;AWS Autogluon：集成了6-12个模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用户可以手动选择模型数量规模。&lt;/p&gt;
&lt;p&gt;与它们不同的是，&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;的模型选择策略试图在给定延迟的情况下选择合适的集合大小，同时最大化准确性。&lt;/p&gt;
&lt;p&gt;2️⃣云上模型服务：&lt;/p&gt;
&lt;p&gt;InFaas、Clipper 、FrugalML、MArk 、Rafiki、 TF-Serving、 SageMaker、AzureML 、Deep-Studio等。&lt;/p&gt;
&lt;p&gt;3️⃣公有云自动缩放：&lt;/p&gt;
&lt;p&gt;现有相关的资源配置策略能分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;多路复用不同的实例类型；&lt;/li&gt;
&lt;li&gt;基于预测策略的主动资源发放。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;使用了类似的负荷预测模型，并在模型集合方面以分布式的方式使用自动缩放虚拟机。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;的自动缩放策略与Swayam[34]的分布式自动缩放策略有相似之处；然而，我们进一步引入了新颖的&lt;strong&gt;重要采样技术&lt;/strong&gt;，以减少未充分利用的模型的过度供应&lt;/p&gt;
&lt;h2 id=&#34;引出cocktail&#34;&gt;引出Cocktail&lt;/h2&gt;
&lt;p&gt;首先回答两个问题：&lt;/p&gt;
&lt;p&gt;1️⃣如何减少资源占用❓&lt;/p&gt;
&lt;p&gt;通过最小化模型集成数量，减少资源使用。文章通过实验，选取精度前50%的模型进行集成。&lt;/p&gt;
&lt;p&gt;完全集成的模型选择是一种过度的行为，而静态集成则会导致精度的损失。这就需要一个动态的模型选择策略，该策略可以根据模型选择策略的准确性和可伸缩性准确地确定所需的模型数量。&lt;/p&gt;
&lt;p&gt;2️⃣如何减少部署成本❓&lt;/p&gt;
&lt;p&gt;大多数云提供商提供瞬态虚拟机，如Amazon Spot实例[69]、谷歌preemptible VMs[9]和Azure Low-priority VMs[7]，可以降低高达10倍的云计算成本。文章**利用这些瞬态VMs(如spot实例)**来大幅降低部署集成模型框架的成本。&lt;/p&gt;
&lt;h2 id=&#34;cocktail整体设计&#34;&gt;Cocktail整体设计&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;架构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515175157818.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515175157818&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Master VM：运行了模型选择算法；1a）来决定将哪些模型集成；1b）被选中的模型加载到缓存中，在相同请求到来时加快响应速度。&lt;/li&gt;
&lt;li&gt;Queries：各个请求分派到不同的实例池。&lt;/li&gt;
&lt;li&gt;Aggregator：用来处理集成模型的返回结果，使用加权多数投票聚合器返回正确的预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了有效地解决资源管理和可伸缩性的挑战，&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;应用多种策略。它维护专用的实例池服务于各个模型，这简化了每个模型的管理和负载平衡开销。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resource Controller：主要管理实例的增减，通过 4a）4b）基于CPU和GPU的开销进行实例数量的管理。&lt;/li&gt;
&lt;li&gt;Load Balancer：将Queries分配给适当的实例，并确保所有获取的实例都被打包到VM中。&lt;/li&gt;
&lt;li&gt;Autoscaler：利用 6a）预测策略为实例池中的实例预测请求负载，确保资源不会被过度配置；同时使用 6b）重要性抽样算法，通过计算每个模型池在给定时间间隔内所服务的请求的百分比来估计每个模型的重要性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;动态模型选择策略&#34;&gt;动态模型选择策略&lt;/h3&gt;
&lt;h4 id=&#34;目标函数&#34;&gt;目标函数&lt;/h4&gt;
&lt;p&gt;本文使用一个基于窗口的动态模型选择策略，使用下面描述的两个&lt;strong&gt;目标函数&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515195954517.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;目标时减小延迟和花费并最大化准确率。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mu_{AL}$： latency-accuracy metric&lt;/li&gt;
&lt;li&gt;$\mu_c$：cost metric&lt;/li&gt;
&lt;li&gt;$Acc_{target}$：目标准确度&lt;/li&gt;
&lt;li&gt;$Lat_{target}$：目标延迟&lt;/li&gt;
&lt;li&gt;$N$：参与集成的模型数量&lt;/li&gt;
&lt;li&gt;$inst_cost$： VM实例的花费&lt;/li&gt;
&lt;li&gt;$m$：指每个模型&lt;/li&gt;
&lt;li&gt;$P_{f_m}$：在单个实例中可以并发执行而不违反延迟指标的推理数量，越大越好&lt;/li&gt;
&lt;li&gt;$k$：常量，取决于VM的性能配置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;第一个目标函数&lt;/strong&gt;$O_1$就是满足$Acc_{target}$和$Lat_{target}$时最大化$\mu_{AL}$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515200819258.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515200819258&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;为此，初始模型列表在满足$Lat_{target}$的模型中选择，并尝试集成使其满足$Acc_{target}$。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Cocktail&lt;/strong&gt;&lt;/em&gt;会将每个模型的准确性作为正确概率，然后迭代地构建一个模型列表，其中它们执行分类的联合概率在准确性目标内。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Acc_{margin}$：为0.2%&lt;/li&gt;
&lt;li&gt;$Lat_{margin}$：为5ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;第二个目标函数&lt;/strong&gt;$O_2$是最小化$\mu_c$。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;该目标调整模型清单的大小，并进一步调整资源采购。因此最大化$P_{f_m}$，最小化$k$。&lt;/p&gt;
&lt;p&gt;对于$N$个模型，每个模型都有一个最小精度，因此选取最小精度前50%的模型，数量为${N\over2} + 1$。来保证集成模型达到预期精度。结果正确率如下：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h4 id=&#34;模型选择和加权多数投票策略&#34;&gt;模型选择和加权多数投票策略&lt;/h4&gt;
&lt;p&gt;为最小化$\mu_c$，设计了一个模型数量缩减策略，只要有超过${N\over2}+1$的模型选择同一种结果，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515205825292.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515205825292&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;资源管理&#34;&gt;资源管理&lt;/h3&gt;
&lt;h4 id=&#34;资源控制器-resource-controller&#34;&gt;资源控制器 Resource Controller&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Resource Types： CPU和GPU实例。GPU实例在&lt;strong&gt;打包大量请求&lt;/strong&gt;执行时是划算的。文章提出自适应打包策略，考虑每个实例的$P_f$ 以及在时间$T$到来的请求数量。只有工作负载匹配$P_f$时，才会将负载分发到对应实例。&lt;/li&gt;
&lt;li&gt;Cost-aware Procurement： 在一个完全封装的实例中执行请求的成本决定了每个实例的开销。在扩展实例之前，需要估计将它们与现有实例一起运行的成本。在时间$T$时，基于预测负载$L_p$和运行实例$R_N$，使用&lt;em&gt;cost-aware greedy&lt;/em&gt;策略来决定要增加的实例数量。&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;Load  Balancer： 在每个模型池中维护一个请求队列，为增加实例池中实例的利用率，负载均衡器将来自队列的每个请求提交到剩余空闲槽位（free slots）。文章使用预期超时10分钟的间隔，来回收实例池中没有被使用的实例。贪婪地分配请求可以使负载较轻的实例更早地伸缩。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;自动伸缩器-autoscaler&#34;&gt;自动伸缩器 Autoscaler&lt;/h4&gt;
&lt;p&gt;我们需要自动伸缩实例数量，来弹性的满足到来的请求负载。&lt;strong&gt;Cocktail&lt;/strong&gt;能准确预测给定时间间隔内的预期负荷。如果需要，&lt;strong&gt;Cocktail&lt;/strong&gt;增加实例到实例池。每隔10秒对SLO违例进行采样，并根据所有实例的资源利用率聚合为每个池生成额外的实例。捕获由于错误预测而导致的SLO违反。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;预测策略：本文设计了DeepARestimator模型。每个模型有1分钟的定期调度间隔$T_s$，在时间$T+T_p$使用预测负责$L_p$，与当前负载$C_p$进行比较，来决定实例数量$I_n$。其中，$T_p$为新实例的平均启动时间。$T_s$设定为1分钟是考虑到AWS  EC2 VMs实例的启动时间。为计算$L_p$，对过去S秒内大小为$W$的相邻窗口的到达率进行采样。使用所有窗口的全局到达率，来预测时间$T$在加减$T_p$时间单元中的$L_p$。$T_p$设置为10分钟，使它有足够的时间来捕捉未来长期的变化。所有这些参数都可以根据系统的需要进行调整。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Importance Sampling： 在自动伸缩中一个重要的问题是模型选择策略为给定的请求约束动态地确定集合中的模型。**基于预测的负载，为每个模型平等地自动伸缩实例，将固有地导致为未充分使用的模型提供过多的实例。**为了解决这个问题，设计了一个加权自动缩放策略，它基于权重智能地为每个池自动缩放实例。算法如下图：&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自动缩放策略如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220515210458324.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220515210458324&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;权重取决于模型被请求(get_popularity)的频率。权重与每个模型池的伸缩实例(launch_workers)的预测负载相乘。这种方法称为Importance Sampling，因为模型池的大小与它们的受欢迎程度成正比。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本论文实验做得非常充分！可以作为范本。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>IEEE CLOUD 21 云上资源管理相关合辑</title>
        <link>https://lizonglingo.github.io/p/ieee-cloud-21-%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3%E5%90%88%E8%BE%91/</link>
        <pubDate>Thu, 21 Apr 2022 14:33:47 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/ieee-cloud-21-%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3%E5%90%88%E8%BE%91/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;本篇整理自IEEE CLOUD&#39;21会议中的文章，主题为云背景下的资源管理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;runwild-resource-management-system-withgeneralized-modeling-for-microservices-on-cloud&#34;&gt;RunWild: Resource Management System withGeneralized Modeling for Microservices on Cloud&lt;/h2&gt;
&lt;h3 id=&#34;摘要&#34;&gt;⭐摘要&lt;/h3&gt;
&lt;h4 id=&#34;问题背景&#34;&gt;问题背景&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;微服务内部通信的复杂性，必须考虑资源利用、调度策略和请求均衡之间的平衡，以防止跨微服务级联的服务质量下降。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;文章做了什么&#34;&gt;文章做了什么&lt;/h4&gt;
&lt;p&gt;提出资源管理系统&lt;strong&gt;RunWild&lt;/strong&gt;，可以控制所有节点涉及到的微服务管理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;扩缩容&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动&lt;/strong&gt;的根据&lt;strong&gt;指定性能表现&lt;/strong&gt;的&lt;strong&gt;负载和性能平衡优化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;统一的&lt;strong&gt;持续部署方案&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;着重强调了&lt;strong&gt;协同metrics感知&lt;/strong&gt;在&lt;strong&gt;预测资源使用和制定部署计划&lt;/strong&gt;中的重要性。&lt;/p&gt;
&lt;p&gt;在IBM云进行实验，&lt;strong&gt;以K8s的自动调度为基线&lt;/strong&gt;，减少P90响应时间11%，增加10%的吞吐率，降低30%的资源使用。&lt;/p&gt;
&lt;h4 id=&#34;贡献&#34;&gt;贡献&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;扩展的部署框架：&lt;strong&gt;适用于K8s的调度框架&lt;/strong&gt;，用来在资源分配、部署、和运行时来控制部署机制；&lt;/li&gt;
&lt;li&gt;通用的建模方法：综合考虑微服务特性、节点的相对独立性、工作负载和全局协同节点状态感知，通过结合聚类和回归技术预测资源使用；&lt;/li&gt;
&lt;li&gt;微服务间交互指标：一个称为内聚的指标反映了在同一个&lt;strong&gt;节点上放置高度相互通信的微服务的优势&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;通过&lt;strong&gt;Service Mesh&lt;/strong&gt;对运行时工作负载进行分区：利用服务网格操作流量路由，用其控制能力来划分工作负载以匹配资源的分配。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;现有技术存在的问题&#34;&gt;⭐现有技术存在的问题&lt;/h3&gt;
&lt;h4 id=&#34;水平伸缩&#34;&gt;水平伸缩&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;超过某个阈值时，实例的增多与性能表现的增长不匹配，正如收益递减定律所解释的那样；&lt;/li&gt;
&lt;li&gt;资源过度分配并不会显著增加性能表现；&lt;/li&gt;
&lt;li&gt;而资源不足会导致性能下降或者致命错误。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;垂直伸缩&#34;&gt;垂直伸缩&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;K8s虽然可以支持HPA和VPA，但是不能一起工作，同时进行HPA和VPA难免会造成干扰。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，复杂的资源依赖，全局的资源调度策略使得伸缩方案受多方面影响。&lt;/p&gt;
&lt;p&gt;文章的动机是&lt;strong&gt;识别、描述和管理所有因素和维度&lt;/strong&gt;，以实现&lt;strong&gt;统一的部署解决方案&lt;/strong&gt;，而不是运行相互干扰的机制。&lt;/p&gt;
&lt;h4 id=&#34;部署的三个角度&#34;&gt;部署的三个角度&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;所部署的服务的实例副本数；&lt;/li&gt;
&lt;li&gt;节点上每个实例所得到的资源；&lt;/li&gt;
&lt;li&gt;每个实例的网络容量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后引出下面4个重要的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调度所涉及到的策略、资源和具体情景很复杂，要考虑的东西太多；&lt;/li&gt;
&lt;li&gt;同一节点上部署的服务可能&lt;strong&gt;对资源的争用很敏感&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务之间的通信，亲和性&lt;/strong&gt;等因素会影响到&lt;strong&gt;全局的服务性能表现、响应事件及吞吐量&lt;/strong&gt;，最好的方式是使部署的微服务&lt;strong&gt;减少跨节点的通信&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;如何将&lt;strong&gt;请求负载均衡到不同实例以带来更好的网络表现&lt;/strong&gt;，虽然Service Mesh能够实现负载的分发，但是难以解决上述问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章列举了一些其他文章做的工作，并对比这些工作解决了上述4个问题中的哪些：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220423213910376.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220423213910376&#34;
	
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该文会精读，请关注最新的文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;fast-and-efficient-performance-tuning-of-microservices&#34;&gt;Fast and Efficient Performance Tuning of Microservices&lt;/h2&gt;
&lt;h3 id=&#34;摘要-1&#34;&gt;⭐摘要&lt;/h3&gt;
&lt;p&gt;针对使用&lt;strong&gt;容器部署的微服务架构应用&lt;/strong&gt;，&lt;strong&gt;以Kubernetes、Docker Swarm容器管理平台为依托&lt;/strong&gt;。在应用正式部署上线之前，也就是在&lt;strong&gt;pre-deployment&lt;/strong&gt;阶段，&lt;strong&gt;迭代的根据资源使用相关指标&lt;/strong&gt;，结合&lt;strong&gt;类多目标优化算法(文章称为heuristic optimization algorithm)&lt;strong&gt;对&lt;/strong&gt;资源分配进行调优&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;系统架构&#34;&gt;⭐系统架构&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ol&gt;
&lt;li&gt;将应用部署到云平台；&lt;/li&gt;
&lt;li&gt;进行负载注入；&lt;/li&gt;
&lt;li&gt;基于Jaeger的监控系统开始进行性能测试和追踪(对每个微服务)，收集数据，如响应时间和资源的使用量；&lt;/li&gt;
&lt;li&gt;通过Jaeger解析服务调用序列；&lt;/li&gt;
&lt;li&gt;由Tuning Agent参照服务序列信息、不同类别请求的响应时间和平均资源使用进行调优；&lt;/li&gt;
&lt;li&gt;Tuning Agent预估每个微服务的新的CPU配额信息；&lt;/li&gt;
&lt;li&gt;将这些信息存储到Tuning数据库中；&lt;/li&gt;
&lt;li&gt;编排器根据这些信息对服务进行迭代部署。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;测量优化的依赖指标&#34;&gt;⭐测量、优化的依赖指标&lt;/h3&gt;
&lt;p&gt;需要对服务进行&lt;strong&gt;请求的注入&lt;/strong&gt;来进行测量，主要指标是&lt;strong&gt;服务响应时间&lt;/strong&gt;。涉及到&lt;strong&gt;链路追踪、性能监控&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;调优模型抽象&#34;&gt;⭐调优模型抽象&lt;/h3&gt;
&lt;h4 id=&#34;小背景前提和假设&#34;&gt;小背景、前提和假设&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用需要一些特定的工作负载$W$&lt;/strong&gt;，这些工作负载发生在特定的情境，例如在线商城的Black Friday。因此，调优过程可以对其他感兴趣的工作负载重放，从而产生一系列特定于工作负载的配置，可以在部署应用程序时适当地使用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本文重点关注CPU资源的限制&lt;/strong&gt;，但是该模型可以拓展到其他资源。&lt;/li&gt;
&lt;li&gt;应用包含**$K$个微服务**，每个微服务运行在自己的container中。&lt;/li&gt;
&lt;li&gt;每个应用支持**$C$种不同的请求类别**。&lt;/li&gt;
&lt;li&gt;每个请求类别**$c$关联到不同的响应时间$T_c$**。&lt;/li&gt;
&lt;li&gt;每类请求**$c$涉及到一个微服务调用序列$S_c$**。&lt;/li&gt;
&lt;li&gt;因此这个序列中每个&lt;strong&gt;微服务$k$都涉及到一个CPU需求&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;主机上对应的&lt;strong&gt;服务$k$所需的CPU配额表示为$\alpha_k$&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;资源分配问题抽象&#34;&gt;资源分配问题抽象&lt;/h4&gt;
&lt;p&gt;问题可以抽象为：在&lt;strong&gt;满足响应时间的需求下，求解对每个微服务CPU配额的最小值&lt;/strong&gt;。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ol&gt;
&lt;li&gt;目标为最小化CPU配额；&lt;/li&gt;
&lt;li&gt;需要满足前提条件，即：资源配额能够使某类请求的响应时间$R_c$小于等于目标值$T_c$；&lt;/li&gt;
&lt;li&gt;其中响应时间$R_c$是工作负载$W$和对$K$个服务CPU配额的函数；&lt;/li&gt;
&lt;li&gt;最后限制CPU需求总额是有限的。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;应用案例及实验&#34;&gt;⭐应用案例及实验&lt;/h3&gt;
&lt;p&gt;使用的微服务案例&lt;strong&gt;Bookstore&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220419153253242.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220419153253242&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;我的问题&#34;&gt;⭐我的问题&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;工作负载的模拟具体如何实现？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有哪些开源微服务应用真正可用又具有一定的代表性？&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;skynet-performance-driven-resource-management-for-dynamic-workloads&#34;&gt;Skynet: Performance-driven Resource Management for Dynamic Workloads&lt;/h2&gt;
&lt;h3 id=&#34;摘要-2&#34;&gt;⭐摘要&lt;/h3&gt;
&lt;h4 id=&#34;问题背景和主要矛盾&#34;&gt;问题背景和主要矛盾&lt;/h4&gt;
&lt;p&gt;云环境下，资源利用率和应用的性能表现之间的矛盾。&lt;/p&gt;
&lt;h4 id=&#34;难点&#34;&gt;难点&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;用户常会&lt;strong&gt;分配过多的资源&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;应用的&lt;strong&gt;多样性和动态性&lt;/strong&gt;，&lt;strong&gt;工作负载的动态性及难以预测性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;性能表现取决于&lt;strong&gt;多种不同资源&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;文章做了什么怎么做的&#34;&gt;文章做了什么，怎么做的&lt;/h4&gt;
&lt;p&gt;提出Skynet，针对上述三个难点，可以自动对云资源进行管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;评估资源需求&lt;strong&gt;依赖的指标&lt;/strong&gt;：Skynet使用performance level objectives(PLOs)准确捕捉用户对所需性能的意图，将用户从资源分配循环中解放。Skynet&lt;strong&gt;通过目标PLO去预估资源需求&lt;/strong&gt;，使用Poportional Integral Derivative(PID)控制器对每个应用调整对应的参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;资源需求&lt;strong&gt;计算、分配、调度方法&lt;/strong&gt;：为捕获每个应用对不同资源依赖，&lt;strong&gt;Skynet扩展了传统的一维PID控制器&lt;/strong&gt;(传统的单输入单输出)，实现对CPU、内存、I/O和网络吞吐的预估。Skynet建立一个动态模型，对于每个应用，将目标PLOs映射到资源，同时考虑多种资源和变化的输入负载。事实上，Skynet处于一个&lt;strong&gt;动态循环控制&lt;/strong&gt;来预估资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实现和评估&lt;/strong&gt;：在&lt;strong&gt;kubernetes中将skynetas实现为端到端的定制调度程序&lt;/strong&gt;，并在5个节点的私有集群和60个裸金属服务器AWS上使用真实的工作负载对其进行评估。以K8s为基线，PLO违规降低7.4倍，资源利用提高两倍。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;系统架构-1&#34;&gt;⭐系统架构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220421114835655.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220421114835655&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;用户可以指定PLOs，明确对吞吐量、延迟、处理时间等指标的需求。Skynet根据这些PLOs，使用PID[41]预估每个应用的资源需求量。动态的将PLO映射到资源需求，这样一来可以让Skynet适应变化的工作负载和每个应用不同的生命阶段。&lt;/p&gt;
&lt;h4 id=&#34;示例&#34;&gt;示例&lt;/h4&gt;
&lt;p&gt;一个web应用PLO为1000请求/秒。Skynet给每个新应用分配一个预定义容器。在执行阶段，Skynet主要使用两个组件：Resource Estimator(RE)和Resource Assigner(RA)，来周期性的调整资源配额以满足PLO：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Skynet周期性监控应用性能指标，如果触发PLO违规，会触发RE。&lt;/li&gt;
&lt;li&gt;RE基于PLO调整PIDs的参数。&lt;/li&gt;
&lt;li&gt;基于目标PLO，RE预估应用新的资源需求。&lt;/li&gt;
&lt;li&gt;当可分配资源满足条件时，RA调整应用容器。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;放置应用以及根据控制器更新应用放置&#34;&gt;放置应用以及根据控制器更新应用放置&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;确定应用资源需求量后，Skynet决定容器的资源限额和放置。具体来说，包括容器打包，节点绑定以及资源配额。其中，容器大小和放置由于需要考虑多种资源的约束，远比打包应用复杂。放置应用的目标是：避免应用间干扰，提高应用性能表现。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当新应用到来时，Skynet进行扫描，查看是否有某个服务节点可以单独满足应用的资源需求，如果不存在这样的服务节点，就迭代执行下列步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加一个容器的数量；&lt;/li&gt;
&lt;li&gt;在容器之间平均分配资源；&lt;/li&gt;
&lt;li&gt;找到能够满足容器需求，并且负载最高的服务节点；&lt;/li&gt;
&lt;li&gt;如果没有，循环执行上述步骤。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调整应用资源配额。每次请求改变资源需求时，有三种可能：(理解的有些别扭？)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;资源不够。该情况下，Skynet决定有没有现存的容器可以移除。然后基于节点负载对节点进行排序，移除额外的容器。&lt;/li&gt;
&lt;li&gt;节点上的可用资源早已被分配给应用。Skynet在容器之间平均增加应用程序的资源，以匹配新的请求。&lt;/li&gt;
&lt;li&gt;可用资源分布在不同的服务节点上。Skynet以放置新应用的思路放置新的容器。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的说，就是处理，&lt;strong&gt;容器应该放置在哪个节点上的问题&lt;/strong&gt;。算法思路如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220421143003087.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220421143003087&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;在kubernetes上的实现&#34;&gt;在Kubernetes上的实现&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220421132506202.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220421132506202&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用Golang实现自定义调度器。使用Prometheus进行监控。代码开源[11]。&lt;/p&gt;
&lt;h3 id=&#34;我的问题-1&#34;&gt;⭐我的问题&lt;/h3&gt;
&lt;h4 id=&#34;关于pid控制理论的补充&#34;&gt;关于PID控制理论的补充&lt;/h4&gt;
&lt;p&gt;已经不止一次在论文中看到使用PID来调整资源分配了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/39573490&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/39573490&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;如果要使用pid算法再细读&#34;&gt;如果要使用PID算法，再细读&lt;/h4&gt;
&lt;p&gt;获得监控数据后具体怎处理？&lt;/p&gt;
&lt;p&gt;分配资源的具体方法？&lt;/p&gt;
&lt;h2 id=&#34;konveyor-move2kube-automatedreplatforming-of-applications-to-kubernetes&#34;&gt;Konveyor Move2Kube: AutomatedReplatforming of Applications to Kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;摘要-3&#34;&gt;⭐摘要&lt;/h3&gt;
&lt;p&gt;文章提出Move2Kube，一个再部署框架，能够自动调整部署细节，并通过部署pipeline&lt;strong&gt;将非Kubernetes平台部署的应用转移到Kubernetes平台上&lt;/strong&gt;，同时最小限度修改应用架构和实现。&lt;/p&gt;
&lt;p&gt;此外，文章提出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个最小化的中间表示，不同的应用部署构建都可以转化到这个中间表示上来。&lt;/li&gt;
&lt;li&gt;一个扩展框架，用于添加对新的部署源平台和目标中间件的支持，同时允许定制化。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Move2Kube已经开源：https://move2kube.konveyor.io/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;要解决什么问题&#34;&gt;要解决什么问题&lt;/h4&gt;
&lt;p&gt;在不是K8s平台部署的应用迁移到K8s平台上，同时应该最小限度的修改原系统的实现和软件架构。&lt;/p&gt;
&lt;h4 id=&#34;挑战难点在哪里&#34;&gt;挑战、难点在哪里&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;应用规模：企业级应用往往有上千个组件，人工迁移费时费力；&lt;/li&gt;
&lt;li&gt;应用异构：多样的部署平台，多样的应用架构和种类；&lt;/li&gt;
&lt;li&gt;不同的代码源、组件仓库：代码源或者使用的组件分布在不同的仓库中，很难将其组织到一起，如何分布的数千个目录中找到正确的文件很有挑战；&lt;/li&gt;
&lt;li&gt;容器化挑战：将应用容器化时，对于优化配置和分层安全很有必要，需要对容器内部、镜像技术和应用配置有深入的理解；&lt;/li&gt;
&lt;li&gt;目标平台映射：找到正确的不同平台的配置映射关系是困难的，例如如何选择从简单的K8s service转换到Istio的配置中；&lt;/li&gt;
&lt;li&gt;应用的最佳实践：K8s有最佳实践[6]，如何确保迁移使用K8s的最佳实践；&lt;/li&gt;
&lt;li&gt;定制化的需求和有效的Day 2 Operation：针对不同应用和需求定制化的配置以适应平台特性需要一定的经验和时间，同时需要考虑Day 2 Operation。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;关于什么是Day 2 Operation：https://jimmysong.io/blog/what-is-day-2-operation/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;move2kube&#34;&gt;Move2Kube&lt;/h4&gt;
&lt;p&gt;这个开源框架旨在解决应用迁移到Kubernetes平台过程中出现的上述问题。它提供了标准化的Pipeline，包括&lt;strong&gt;容器化、参数化、配置优化、定制化&lt;/strong&gt;等解决方案，满足面向&lt;strong&gt;特定平台的多源、多服务&lt;/strong&gt;的应用部署迁移。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该文不太属于资源管理方面。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>Characterizing microservice dependency and performance: Alibaba trace analysis</title>
        <link>https://lizonglingo.github.io/p/characterizing-microservice-dependency-and-performance-alibaba-trace-analysis/</link>
        <pubDate>Tue, 29 Mar 2022 13:40:56 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/characterizing-microservice-dependency-and-performance-alibaba-trace-analysis/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;来源： SoCC&#39;21&lt;/p&gt;
&lt;p&gt;作者： Alibaba Group&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;http://cloud.siat.ac.cn/pdca/socc2021-AlibabaTraceAnalysis.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://cloud.siat.ac.cn/pdca/socc2021-AlibabaTraceAnalysis.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;h3 id=&#34;背景&#34;&gt;背景&lt;/h3&gt;
&lt;p&gt;理解微服务的特征，对利用微服务架构的特性很重要。然而，目前还没有对微服务及其相关系统在生产环境下的全面研究。&lt;/p&gt;
&lt;h3 id=&#34;工作&#34;&gt;工作&lt;/h3&gt;
&lt;p&gt;我们对阿里巴巴集群中大规模部署微服务进行了详实的分析。研究重点是&lt;strong&gt;描述微服务的依赖关系及其运行时性能&lt;/strong&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对微服务调用图进行了深入剖析，以量化它们与数据并行作业的传统DAG之间的区别&lt;/li&gt;
&lt;li&gt;为合成更有代表性的微服务追踪轨迹，构建了数学模型去模拟调用图&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;结论&#34;&gt;结论&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;通过分析，发现调用图是重尾分布的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;它们的&lt;strong&gt;拓扑结构类似于树&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;而且许多微服务都是热点(hot-spots)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;发现三类有意义的调用依赖，可以&lt;strong&gt;用来优化微服务的设计&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;大多数&lt;strong&gt;微服务对CPU受到的干扰比对内存受到的干扰更加敏感&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;分析目标有20000个微服务，时间是7天，表征了它们的特性，包括动态的调用图，表征了微服务间调用依赖还有运行时性能分析。&lt;/p&gt;
&lt;h4 id=&#34;微服务调用图与传统的数据并行处理任务dag图明显不同&#34;&gt;微服务调用图与传统的数据并行处理任务DAG图明显不同&lt;/h4&gt;
&lt;p&gt;虽然可以看作有向图，但是有下面几个明显的不同：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调用图的大小遵循重尾分布，有10%的调用图包含超过40个微服务生命阶段，其他大多数只有几个阶段（&lt;strong&gt;因为由于服务运行时的动态性，不同时刻的调用图不同&lt;/strong&gt;）&lt;/li&gt;
&lt;li&gt;调用图的形状像树，大部分节点只有一条入边，与大数据任务的明显不同&lt;/li&gt;
&lt;li&gt;存在微服务热点，&lt;strong&gt;5%的微服务被90%的微服务调用&lt;/strong&gt;，而传统的DAG图中不会存在节点共享的情况&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务在运行时有高度动态的调用关系&lt;/strong&gt;，在极端的情况下，同一个在线服务有超过9类拓扑上不同的图&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;微服务的强依赖关系为优化微服务设计提供了方向&#34;&gt;微服务的强依赖关系为优化微服务设计提供了方向&lt;/h4&gt;
&lt;p&gt;例如，优化两个强依赖关系的调用接口，提升上游、下游微服务间的通信效率，避免形成局部的性能瓶颈。&lt;/p&gt;
&lt;h4 id=&#34;微服务对cpu扰动比对内存扰动更加敏感&#34;&gt;微服务对CPU扰动比对内存扰动更加敏感&lt;/h4&gt;
&lt;p&gt;CPU扰动会严重影响到响应时间。例如，CPU利用率在10%和30%时，响应时间相差20%。&lt;/p&gt;
&lt;p&gt;同时，这表明需要对有效的任务调度策略有极大的需求，以平衡不同机器上CPU的利用率。&lt;/p&gt;
&lt;h4 id=&#34;随机模型可以很好地模拟动态微服务调用图&#34;&gt;随机模型可以很好地模拟动态微服务调用图&lt;/h4&gt;
&lt;p&gt;现有的一些调用图不能展现服务运行时的调用状态，在整个生命周期中不会跟随请求的发生而动态改变。&lt;/p&gt;
&lt;p&gt;为此，文章构建了一个随机模型，通过对微服务进行分类来生成调用图。&lt;/p&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;首次对生产集群中的微服务进行详细的研究，包括微服务调用图的结构和依赖属性&lt;/li&gt;
&lt;li&gt;对微服务运行时表现进行详细的表征，对微服务调度和资源管理给出深入的洞见&lt;/li&gt;
&lt;li&gt;构建图模型来有效的生成大规模的微服务追踪结构，对在模拟图中表征结构性质进行了理论分析&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;microservice-background-and-alibaba-trace-overview&#34;&gt;Microservice Background And Alibaba Trace Overview&lt;/h2&gt;
&lt;h3 id=&#34;微服务架构&#34;&gt;微服务架构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220327210505343.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220327210505343&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;追踪概览&#34;&gt;追踪概览&lt;/h3&gt;
&lt;p&gt;收集了超过100亿次调用的追踪。&lt;/p&gt;
&lt;h4 id=&#34;物理运行环境&#34;&gt;物理运行环境&lt;/h4&gt;
&lt;p&gt;使用Kubernetes管理裸金属云环境，依赖硬件优化加强集群性能表现，并实现不同服务间的隔离性。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220327210843935.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220327210843935&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在线服务和离线服务会存在同一个裸金属节点上，以加强资源利用率&lt;/li&gt;
&lt;li&gt;在线服务运行在容器中，直接被K8s所管理，而离线任务被分配定量的pods，交给调度器进行调度&lt;/li&gt;
&lt;li&gt;批处理任务被放到安全容器中来保证安全性&lt;/li&gt;
&lt;li&gt;有状态应用被部署在专门的节点上，不和其他无状态或是批处理应用共享机器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;微服务系统测量&#34;&gt;微服务系统测量&lt;/h4&gt;
&lt;p&gt;如Figure 2(b)，微服务监控对每个容器的指标进行测量，隔段时间再求均值。测量的范围从硬件层面的缓存miss率，到操作系统层面的CPU利用率还有内存使用等，还有容器应用层面的例如JVM的堆使用和垃圾回收。&lt;/p&gt;
&lt;p&gt;测量用Timestamp来作为时间序列的表征。&lt;/p&gt;
&lt;h4 id=&#34;调用图中的微服务调用指标&#34;&gt;调用图中的微服务调用指标&lt;/h4&gt;
&lt;p&gt;如Figure 2(c)：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;微服务调用通过TraceID作为请求标识，代表一个调用图&lt;/li&gt;
&lt;li&gt;interface是上游服务和下游服务的调用接口&lt;/li&gt;
&lt;li&gt;还记录了上下游服务Pod的IP&lt;/li&gt;
&lt;li&gt;上下游调用的响应时间&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;聚合调用&#34;&gt;聚合调用&lt;/h4&gt;
&lt;p&gt;如Figure 2(d)，微服务的调用情况也会被记录：调用时间戳、响应时间、微服务、调用和被调用的接口、上下游微服务。&lt;/p&gt;
&lt;h2 id=&#34;解析调用图&#34;&gt;解析调用图&lt;/h2&gt;
&lt;h3 id=&#34;微服务调用图的特性&#34;&gt;微服务调用图的特性&lt;/h3&gt;
&lt;p&gt;**微服务调用图的数量呈现重尾分布。**大多数调用图只包含少数微服务，调用层级不超过3层。但有的调用图包含大量的微服务和很深的层次，如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;图中的微服务数量呈现Burr分布&lt;/li&gt;
&lt;li&gt;超过10%的服务包含超过40个不同的微服务&lt;/li&gt;
&lt;li&gt;超过40个微服务的大型应用中，大约有50%的微服务是缓存服务（因为系统越庞大，需要缓存来加快系统响应时间，使用缓存比数据库有更高的效率）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图是调用图深度的分布：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;平均深度为4.27层&lt;/li&gt;
&lt;li&gt;深度在3层的服务居多&lt;/li&gt;
&lt;li&gt;仍有超过4%的调用图超过10层&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所带来的问题是，&lt;strong&gt;如何使用深度学习方法，为这些服务分配正确的容器数量&lt;/strong&gt;，如[17, 40]。这些方法将不同层次的微服务进行编码，并将实时的资源分配作为神经网络的输入向量。但是这样很容易产生过拟合现象，调用图规模太大，而用于调整模型的负标签太少，如RT violation现象。&lt;/p&gt;
&lt;p&gt;因此，&lt;strong&gt;急需找到合适的方法有效的为大规模微服务分配资源&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;微服务调用图像一棵树，许多图包含一条较长的链&lt;/strong&gt;。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220328112852049.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220328112852049&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当微服务数量不断增加，调用深度逐渐稳定&lt;/li&gt;
&lt;li&gt;如果一个微服务调用涉及到有状态服务，那么一般来说这条调用链不会再延长，终止于有状态服务&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于调用图节点的出边和入边分布如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220328113313961.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220328113313961&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;超过10%的有状态服务至少有5条出边&lt;/li&gt;
&lt;li&gt;大多数微服务只有一条入边&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一旦微服务层数大于2时，相关层包含的微服务一般只有一个，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220328120224056.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220328120224056&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;例如，当层数在10层时，有超过50%的情况是这层只有一个微服务。也就是上面提到的，&lt;strong&gt;为什么深度越深，就越是表现出链式结构&lt;/strong&gt;。此外，这种情况也利于查找微服务调用中出现的瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;无状态服务容易成为hot-spots&lt;/strong&gt;。如上面的Figure 4所示，超过5%的微服务在聚合调用后有超过16条入边，这些超级微服务承载力将近90%的调用，涉及到95%的调用。因此这种&lt;strong&gt;松耦合架构展现了严重的负载不平衡&lt;/strong&gt;。这有利于资源扩展，因为系统管理员应该只关注单个微服务的扩展，并为这些超级微服务分配更多的容器。&lt;/p&gt;
&lt;p&gt;**微服务调用图有很高的动态性。**即使是同一个在线应用，它们生成的调用拓扑图都有显著的差异。例如同一个付款请求，一个有优惠券的用户和一个会员用户或者一个普通用户，请求的调用的微服务都有显著的不同。如下图，所有在线服务都至少有两类图拓扑结构：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220328122519395.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220328122519395&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;另外，还有超过10%的微服务呈现9中调用模式。**这进一步给基于图的微服务预测任务带来了巨大的挑战。**现有的基于CNN的微服务资源管理方法不能描述这些动态特性，也不适用于实际的工业应用。&lt;/p&gt;
&lt;h3 id=&#34;图学习算法&#34;&gt;图学习算法&lt;/h3&gt;
&lt;p&gt;该算法的目的在于：将调用图中的微服务进行分类。&lt;strong&gt;关键点是将每个微服务转换为一个向量&lt;/strong&gt;。如InfoGraph算法，这是无监督学习，它将节点信息（如某种微服务），还有边信息（如微服务的调用关系），做成邻接矩阵作为深度神经网络的输入。&lt;/p&gt;
&lt;p&gt;通过综合训练集的信息，InfoGraph可以为每个图生成一个嵌入向量。&lt;/p&gt;
&lt;p&gt;文章分别训练每个在线服务，并在嵌入的20维向量上使用K-means聚类，将该服务生成的所有调用图分组为多个类。聚类的数量在[2,10]中，&lt;strong&gt;并用来生成平均轮廓系数&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在聚类后，使用通用方法Graph Kernel，用来生成两个图间的相似度。&lt;/p&gt;
&lt;p&gt;对图进行聚类和相似判断的算法如下：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;详细分析&#34;&gt;详细分析&lt;/h3&gt;
&lt;h4 id=&#34;无状态微服务的调用模式在不同的层上有很大的不同&#34;&gt;无状态微服务的调用模式在不同的层上有很大的不同&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;通常无状态服务没有下游微服务，调用图一般不会在无状态服务处继续扩展&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;消息队列对减少深层次的调用图的端到端时延很有帮助&#34;&gt;消息队列对减少深层次的调用图的端到端时延很有帮助&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;对于依赖缓存的服务，当缓存未命中时，会花费大量时间调用数据库服务&lt;/li&gt;
&lt;li&gt;当深度增加时，无状态微服务和数据库(即S2D)之间的通信百分比呈亚线性增长&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图中：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;图a展示不同类型的无状态服务对调用层数的影响&lt;/li&gt;
&lt;li&gt;图b展示了随着深度增加，服务间通信类型，请求类型的占比&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;无状态服务间依赖&#34;&gt;无状态服务间依赖&lt;/h2&gt;
&lt;p&gt;上文中提到，无状态服务过多依赖其他的存储服务，一些时候不可避免的带来服务资源占用。通过研究无状态服务间依赖以避免通信过载和死锁的发生。&lt;/p&gt;
&lt;h3 id=&#34;循环依赖关系&#34;&gt;循环依赖关系&lt;/h3&gt;
&lt;p&gt;下图为循环依赖关系的简单实例：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;这种依赖分为：强依赖关系和弱依赖关系。强循环依赖如果设计不合理，会导致死锁。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强循环依赖关系：上游的输入接口与下游的应答接口相同，直接的就是I1=I3&lt;/li&gt;
&lt;li&gt;弱循环依赖关系：I1 != I3&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;循环依赖关系在调用图同不可忽视&#34;&gt;循环依赖关系在调用图同不可忽视&lt;/h4&gt;
&lt;p&gt;下图展示循环依赖的占比以及这些依赖使用的通信方式：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;大多数通信模式使用RPC进行&lt;/li&gt;
&lt;li&gt;在所有的循环依赖中，2.7%是强依赖关系&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;耦合依赖高频率的调用次数和长调用时间&#34;&gt;耦合依赖：高频率的调用次数和长调用时间&lt;/h3&gt;
&lt;p&gt;对于调用率和调用次数的计算如下：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Count(X)：上游Y调用下游M的次数（相邻两层中，X可能会被Y多次调用）&lt;/p&gt;
&lt;p&gt;Sum：表示所有调用图中由Y触发的两层调用的数量&lt;/p&gt;
&lt;p&gt;N：为两层调用中X被调用的数量&lt;/p&gt;
&lt;p&gt;如果Call Probability和Call Time的值超过2和0.9，就说这两个服务是耦合依赖的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;对于有强耦合依赖的服务，可以将它们的接口做到一起以优化，减轻网络拥塞&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;并行依赖&#34;&gt;并行依赖&lt;/h3&gt;
&lt;p&gt;并行依赖可以减轻上游服务的响应时间。但是在这种情况下建议将服务做进一个微服务中。&lt;/p&gt;
&lt;h2 id=&#34;微服务运行时表现&#34;&gt;微服务运行时表现&lt;/h2&gt;
&lt;p&gt;理解微服务运行时的情况有利于保证服务质量。在本节中，文章研究图拓扑和资源干扰以及微服务调用率(MCR)对响应时间的影响。&lt;/p&gt;
&lt;h3 id=&#34;微服务调用率&#34;&gt;微服务调用率&lt;/h3&gt;
&lt;p&gt;MCR记录了每个容器每分钟接收调用的次数。如果MCR过大，可能意味着资源紧张。&lt;/p&gt;
&lt;p&gt;使用Spearman相关系数来评估MCR序列和微服务容器运行时的序列。如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220329130343180.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220329130343180&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上图展示了MCR和多种不同的系统层面和应用层面的资源调用的累积分布。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;微服务调用率与CPU利用率和Young GC高度相关，但与内存利用率无关&lt;/li&gt;
&lt;li&gt;说明CPU利用率和Young GCs最能反应资源紧张程度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;微服务响应时间表现&#34;&gt;微服务响应时间表现&lt;/h3&gt;
&lt;p&gt;本部分研究调用图的复杂性、资源竞争和MCR以及其他因素对微服务响应时间表现的影响。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;端到端时延在拓扑结构类似的调用图中较为稳定，而在拓扑结构不同的调用图中显得很大不同。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过上面的图聚类算法将调用图进行分类，对每类图中的响应时间进行计算。这进一步说明图拓扑结构对端到端RT有很大的影响。此外，文章&lt;strong&gt;设计的图学习算法可以用于预测RT性能。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;端到端性能会由于CPU的高占用而下降。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;**在微服务调用率变化时，时延并没有太大波动，**这是由于在集群中及时处理了调用请求避免消息堆积。&lt;/p&gt;
&lt;h2 id=&#34;用概率模型生成微服务图&#34;&gt;用概率模型生成微服务图&lt;/h2&gt;
&lt;p&gt;涉及到的算法暂时没看太懂&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;相关工作&#34;&gt;相关工作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Microservice benchmarks&lt;/li&gt;
&lt;li&gt;Serverless benchmarks&lt;/li&gt;
&lt;li&gt;Cloud workloads&lt;/li&gt;
&lt;li&gt;Cloud trace analysis&lt;/li&gt;
&lt;li&gt;Performance characterization of online services&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>MLaaS in the Wild: Workload Analysis and Scheduling in Large-Scale Heterogeneous GPU Clusters</title>
        <link>https://lizonglingo.github.io/p/mlaas-in-the-wild-workload-analysis-and-scheduling-in-large-scale-heterogeneous-gpu-clusters/</link>
        <pubDate>Mon, 21 Mar 2022 21:05:51 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/mlaas-in-the-wild-workload-analysis-and-scheduling-in-large-scale-heterogeneous-gpu-clusters/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;来源： NSDI&#39;22&lt;/p&gt;
&lt;p&gt;作者：Alibaba Group&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;p&gt;在ML as a Service中，数据中心为ML提供算力保证。而多样的ML工作负载面对&lt;strong&gt;异构GPU集群&lt;/strong&gt;时会出现一些问题。通过两个月的数据收集，采集了超过&lt;strong&gt;6000个GPU&lt;/strong&gt;的生产数据。并发现集群调度面临的一些问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;低GPU利用率&lt;/li&gt;
&lt;li&gt;长队列延迟&lt;/li&gt;
&lt;li&gt;需要高端GPU的任务调度难度大&lt;/li&gt;
&lt;li&gt;异构机器负载不均衡&lt;/li&gt;
&lt;li&gt;CPU潜在的瓶颈问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;文章对上述问题提供了一些解决方案。&lt;/p&gt;
&lt;p&gt;本文的最大贡献是提供了一个真实的大规模生产级别的ML集群的追踪数据，并在此基础之上进行分析，为ML as a Service - 云环境下的ML工作负载调度提供了重要的一手数据。&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;在ML框架下的任务需要不同的调度策略，例如GPU局部性、群调度，而且需要调配跨数量级的资源。同时集群中的机器是异构的，一些配置如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220320210534789.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220320210534789&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;而异构的运行环境给资源管理和调度带来新的困难。&lt;/p&gt;
&lt;h4 id=&#34;gpu碎片化使用带来的低利用率&#34;&gt;GPU碎片化使用带来的低利用率&lt;/h4&gt;
&lt;p&gt;例如一个任务实例只使用GPU资源的一部分。流处理程序的GPU利用率的中位数值只有0.042GPU。&lt;strong&gt;粗粒度的GPU分配&lt;/strong&gt;使得资源使用率低下。&lt;/p&gt;
&lt;p&gt;为解决这个问题，文章提出了&lt;strong&gt;GPU sharing&lt;/strong&gt;，一种可以以&lt;strong&gt;时分复用的方式让多个任务共享GPU&lt;/strong&gt;的控制方式。使用该方式，将许多低GPU的工作负载整合起来，使用一个GPU，提高资源使用效率。此外，这种共享方式&lt;strong&gt;不会引起资源争用干扰&lt;/strong&gt;，资源竞争的概率十分小。&lt;/p&gt;
&lt;h4 id=&#34;短任务面临的长排队延迟&#34;&gt;短任务面临的长排队延迟&lt;/h4&gt;
&lt;p&gt;短时间运行的任务实例容易由于队列头阻塞而导致长队列延迟，&lt;strong&gt;大约9%的任务排队等待的时间超过他们的执行时间&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;一种有效的解决方案是&lt;strong&gt;预测任务运行时间&lt;/strong&gt;，&lt;strong&gt;并将短任务优先级尽可能提高&lt;/strong&gt;，避免与长任务竞争。通过仔细的特征工程，我们可以预测大多数重复任务的持续时间，误差在25%以内，这足以根据之前的工作建议做出质量调度决策（因为，通过观察，集群中有65%的任务有重复的工作负载）。跟踪驱动的仿真结果表明，通过预测任务持续时间采用最短作业优先调度，平均完成时间减少63%以上。&lt;/p&gt;
&lt;h4 id=&#34;高gpu使用的作业难以进行调度&#34;&gt;高GPU使用的作业难以进行调度&lt;/h4&gt;
&lt;p&gt;集群中的&lt;strong&gt;一些任务要求无共享的使用GPU，以利用高级硬件特性，达到加速训练的目的&lt;/strong&gt;，如NVLink[12]，因此，对这些任务难以进行调度。&lt;/p&gt;
&lt;p&gt;集群中的调度器使用一个简单的 reserving-and-packing 策略在集群中分辨出这样的任务。它&lt;strong&gt;保留高端的GPU机器&lt;/strong&gt;，如，V100 with NVLinks，用于少数具有挑剔调度要求的高GPU任务，同时将其他工作负载打包到不太高级的机器上，使用GPU共享策略&lt;strong&gt;保证资源的利用率&lt;/strong&gt;。此外，该策略还&lt;strong&gt;提升了平均队列等待延迟，加快了任务调度&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;负载不均衡&#34;&gt;负载不均衡&lt;/h4&gt;
&lt;p&gt;明显的是，&lt;strong&gt;低端GPU比高端GPU更加拥挤&lt;/strong&gt;，前者被分配了70%的GPU和CPU资源，而后者只被分配35%的CPU和49%的GPU资源。&lt;/p&gt;
&lt;p&gt;工作负载和机器之间也存在供应不匹配的问题。例如，工作在8GPU的工作负载对CPU的需求是那些可以提供12GPU的1.9倍，简而言之就是，那些&lt;strong&gt;性能更弱的机器被分配了与其能力不匹配的工作负载&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;cpu瓶颈&#34;&gt;CPU瓶颈&lt;/h4&gt;
&lt;p&gt;一些机器学习、深度学习作业不仅仅需要GPU，有的也需要CPU资源，这造成CPU瓶颈。同时发现， 工作在高CPU利用率机器上的任务容易减速。&lt;/p&gt;
&lt;h2 id=&#34;工作负载特征分析&#34;&gt;工作负载特征分析&lt;/h2&gt;
&lt;h3 id=&#34;追踪概述&#34;&gt;追踪概述&lt;/h3&gt;
&lt;p&gt;关于数据集的数据内容和下载请查看&lt;a class=&#34;link&#34; href=&#34;https://github.com/alibaba/clusterdata&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;clusterdata&lt;/a&gt;。实际上并不能明确的知道容器里执行的到底是什么类型的训练任务，但是可以从作业名中得到一些线索。&lt;/p&gt;
&lt;p&gt;下图为PAI和Trace的架构：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h4 id=&#34;jobs-tasks-and-instances&#34;&gt;Jobs, tasks, and instances&lt;/h4&gt;
&lt;p&gt;用户提交jobs，一个job有一个或多个tasks来扮演不同的计算角色，每个task使用Docker运行一个或多个instances。&lt;/p&gt;
&lt;p&gt;例如，一个分布式训练job有一个参数服务task，该task有两个实例，此外还有一个worker task有10个实例。一个task的所有instances有相同的资源需求，并且需要gang-schedule。&lt;/p&gt;
&lt;p&gt;我们主要&lt;strong&gt;关注任务实例，也就是instance这一级别的工作&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;heavy-skewed-instance-distribution&#34;&gt;Heavy-skewed instance distribution&lt;/h4&gt;
&lt;p&gt;PAI追踪了120万个tasks，超过750万个instances，由超过1300个用户提交。下图展示了用户提交的task instance的分布，表现出严重的不平衡：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;5%的用户提交了大概77%的task instances，大概每个用户运行1.75万instance。而50%的用户每人运行少于180个instances。&lt;/p&gt;
&lt;h4 id=&#34;the-prevalence-of-gang-scheduling&#34;&gt;The prevalence of gang-scheduling&lt;/h4&gt;
&lt;p&gt;分布式的任务需要gang-schedule（认为超过2个GPU的调度），如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;大约85%的任务需要这样的需求，有20%的任务需要超过100个GPU的调度，还有的甚至要进行超过1000个GPU的调度。&lt;/p&gt;
&lt;h4 id=&#34;gpu-locality&#34;&gt;GPU locality&lt;/h4&gt;
&lt;p&gt;除了gang-schedule，一个任务可能会在同一台机器上的多个GPU上运行它的所有实例，也就是存在&lt;strong&gt;GPU局部性&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;虽然这种情况会引发调度延迟的加剧（一些任务等待调度的时间延长），但是在单节点的GPU上进行训练减少了GPU to GPU的通信时间。&lt;/p&gt;
&lt;p&gt;但是通过增强GPU局部性，可以&lt;strong&gt;让某些任务的训练速度加快10倍&lt;/strong&gt;。&lt;/p&gt;
&lt;h4 id=&#34;gpu-sharing&#34;&gt;GPU sharing&lt;/h4&gt;
&lt;p&gt;GPU sharing利用时分复用的原理使得多用户可共享一个GPU进行训练。&lt;/p&gt;
&lt;h4 id=&#34;various-gpu-types-to-choose-from&#34;&gt;Various GPU types to choose from&lt;/h4&gt;
&lt;p&gt;PAI提供异构的GPU可供任务选择。在集群中只有6%的训练任务需要运行在特定的GPU上，其他的任务则对GPU类型没有限制。&lt;/p&gt;
&lt;h3 id=&#34;时间模型&#34;&gt;时间模型&lt;/h3&gt;
&lt;p&gt;从时间角度来观察PAI工作负载。&lt;/p&gt;
&lt;h4 id=&#34;diurnal-task-submissions-and-resource-requests&#34;&gt;Diurnal task submissions and resource requests&lt;/h4&gt;
&lt;p&gt;下图是一周中task和instance的提交情况，还有总体的资源请求情况：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220321104136979.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220321104136979&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;从中可以得到以下几点信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;周中提交数量多余周末&lt;/li&gt;
&lt;li&gt;夜晚也有任务提交的高峰&lt;/li&gt;
&lt;li&gt;大多数在夜间提交的任务并非计算密集型&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;instance-run-time-in-a-wide-range&#34;&gt;Instance run-time in a wide range&lt;/h4&gt;
&lt;p&gt;下图展示了instance运行时间的分布：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;运行时间的变化范围很大，有4个数量级&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;non-uniform-queueing-delays&#34;&gt;Non-uniform queueing delays&lt;/h4&gt;
&lt;p&gt;理解为在队列中等待调度的时间。这段时间指task提交到instance执行的时间，如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;对比long-task，short-task通常花费更多比例的时间在等待调度上&lt;/li&gt;
&lt;li&gt;大约9%的短作业实例花费超过完成时间的一半去等待调度，而长左右这个数值只有3%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，task instance的队列延迟还取决于GPU的需求，如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;那些可以共享GPU的instance（GPU需求为0-1）可以更快的被调度，其等待调度的等待时间P90值为497s&lt;/li&gt;
&lt;li&gt;而不支持共享GPU的任务的这一值为1150&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时，长队列等待时间也出现在一些需要高端GPU的任务中，如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;例如在V100和V100M32上的instance需要更多等待时间&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;空间模型&#34;&gt;空间模型&lt;/h3&gt;
&lt;p&gt;通过分析资源请求和使用，分析了PAI task instance的空间模型。每15s进行一次测量，并使用虚拟化工具[2, 25]去分析用户的负载模式和他们的资源需求。&lt;/p&gt;
&lt;h4 id=&#34;heavy-tailed-distribution-of-resource-requests&#34;&gt;Heavy-tailed distribution of resource requests&lt;/h4&gt;
&lt;p&gt;如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;图5(a)(b)(c)中的蓝色实现表示，大约20%的实例占用了80%的资源，而其余的只要很少一部分资源&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;以P95和中位数比较，P95需要12vCPU、1GPU、59GB内存，而中位数只要6vCPU、0.5GPU和29GB内存。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;uneven-resource-usage-low-on-gpu-but-high-on-cpu&#34;&gt;Uneven resource usage: Low on GPU but high on CPU&lt;/h4&gt;
&lt;p&gt;集群中instance的资源使用中位数在1.4vCPU、0.042GPU和3.5GB内存，远小于资源请求的中位数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;观察到存在GPU空闲和CPU不够用的情况，并推断GPU的低利用率不是因为对GPU的需求少，而是CPU瓶颈限制了GPU的使用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从5(b)中也可以看到，对GPU的实际使用远小于GPU资源的需求&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从5(d)中，对应X坐标&amp;gt;1的值表示CPU的使用量大于申请的量，有19%的instance出现这种情况，而超量使用GPU的只有约3%的实例，对内存来说这一值也只有9%&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gpu利用率&#34;&gt;GPU利用率&lt;/h2&gt;
&lt;h3 id=&#34;计算资源利用率&#34;&gt;计算资源利用率&lt;/h3&gt;
&lt;p&gt;包括CPU、GPU和Memory。监控系统每15s收集数据，并存到时间序列数据库中。&lt;/p&gt;
&lt;p&gt;如下图：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;相比内存来说，GPU和CPU利用率普遍高，也说明大部分任务不是内存集中型&lt;/li&gt;
&lt;li&gt;GPU利用率的P90跨度很广，这与GPU使用有突发性相关，同时也与调度策略有关&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;网络和io的低利用率&#34;&gt;网络和I/O的低利用率&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;网络中数据接收量普遍较低&lt;/li&gt;
&lt;li&gt;网络带宽普遍不能达到指定数值(如不能达到保证的10Gbps、32Gbps)&lt;/li&gt;
&lt;li&gt;iowait模式比usr和kernel模式少三个数量级，这意味着CPU大多数时间在进行计算而不是在等待I/O&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;优化集群管理的方向&#34;&gt;优化集群管理的方向&lt;/h2&gt;
&lt;p&gt;在PAI中，集群管理有两个优化目标：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实现GPU的高利用率&lt;/li&gt;
&lt;li&gt;缩短task的运行时间&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;gpu共享&#34;&gt;GPU共享&lt;/h3&gt;
&lt;p&gt;与CPU不同，GPU天生就没有共享特性。PAI以&lt;strong&gt;时分复用和空分复用（内存）方式&lt;/strong&gt;，使多个任务实例可以共享一个GPU。&lt;/p&gt;
&lt;h4 id=&#34;benefits-of-gpu-sharing&#34;&gt;Benefits of GPU sharing&lt;/h4&gt;
&lt;p&gt;下图将是否使用GPU sharing的表现进行对比：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;平均而言，共享只需要50%的GPU资源，可节省高达73%的费用，节省大量的GPU资源&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;does-gpu-sharing-cause-contention&#34;&gt;Does GPU sharing cause contention?&lt;/h4&gt;
&lt;p&gt;随着利用率的增加，运行在共享GPU上的实例开始争夺资源。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;由于大多数高利用率的GPU上面运行单个实例(平均4.5%的GPU运行多个实例)，因此不会发生争用，所以认为GPU共享不会在集群中引起严重的争用&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;预测重复任务的持续时间&#34;&gt;预测重复任务的持续时间&lt;/h3&gt;
&lt;p&gt;文章认为预测ML认为实例的运行时间是实现更好调度的关键。现存的预测方案基于迭代次数、损耗曲线、目标精度和训练速度等指标。&lt;/p&gt;
&lt;h4 id=&#34;the-prevalence-of-recurring-tasks&#34;&gt;The prevalence of recurring tasks&lt;/h4&gt;
&lt;p&gt;文章发现大多数任务都是重复的，并且它们的实例运行时可以很好地从过去的执行中预测出来。通过对任务的元数据，如：脚本、命令行参数、数据源和输出，进行hash得到Group tag来标识重复的任务。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;约65%的任务在trace中至少运行5轮&lt;/li&gt;
&lt;li&gt;大多数重复任务每个周期都有相似的运行时间&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;instance-duration-prediction-for-recurring-tasks&#34;&gt;Instance duration prediction for recurring tasks&lt;/h4&gt;
&lt;p&gt;实际的预测使用三个特征作为输入：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;task&amp;rsquo;s username - User&lt;/li&gt;
&lt;li&gt;resource request - Resource including GPU and other resources&lt;/li&gt;
&lt;li&gt;group tag - Group&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;利用上述特征，基于CART(Classification And Regression Tress)算法预测实例的平均运行时间。作为评估，使用至少重复5轮的任务，下图为预测详情：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;从上图得知：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Group是重要指标&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;benefits-for-scheduling&#34;&gt;Benefits for scheduling&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220417180125631.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220417180125631&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;上图展示不同调度方法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SJF-Oracle明显好于其他算法，该算法基于真实的任务持续时间和预测算法&lt;/li&gt;
&lt;li&gt;给的特征越多，效果越好&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;调度面临的挑战&#34;&gt;调度面临的挑战&lt;/h2&gt;
&lt;p&gt;本部分使用两个案例：典型的ML tasks，分别有高/低GPU资源需求的特性。&lt;/p&gt;
&lt;h3 id=&#34;高gpu需求任务的研究&#34;&gt;高GPU需求任务的研究&lt;/h3&gt;
&lt;p&gt;集群中一些任务有计算密集型实例，需要很高的GPU资源。&lt;/p&gt;
&lt;h4 id=&#34;nlp-with-advanced-language-models&#34;&gt;NLP with advanced language models&lt;/h4&gt;
&lt;p&gt;NLP任务中，73%的有大规模的输入，需要16GB或更高的内存。下图展示了NLP实例对GPU资源的需求和使用情况：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;约40%的实例需要超过1个GPU，超过那些常规的任务&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;image-classification-with-massive-output&#34;&gt;Image classification with massive output&lt;/h4&gt;
&lt;p&gt;集群中还有些任务需要GPU to GPU的高效率通信，GPU局部性可以提高通信效率。典型代表就是图像分类模型，其中存在规模庞大的全连接层，要求在工作实例之间进行大量的梯度更新，需要使用大量的通信资源，使通信成为瓶颈。&lt;/p&gt;
&lt;p&gt;如图14(b)中，启用NVLink极大缩短了任务的运行时间。&lt;/p&gt;
&lt;h3 id=&#34;低gpu需求的任务研究&#34;&gt;低GPU需求的任务研究&lt;/h3&gt;
&lt;p&gt;使用三种广泛使用的任务进行研究。一些CPU密集型的任务可能会导致GPU的利用率低下。&lt;/p&gt;
&lt;h4 id=&#34;ctr-prediction-model-training-and-inference&#34;&gt;CTR prediction model training and inference&lt;/h4&gt;
&lt;p&gt;在追踪中，有6.7%的广告点击率预测系统（ advertisement click- through rate (CTR) prediction）使用了CTR模型。其中有25%的实例负责训练，75%的实例负责推理工作。这些实例的CPU和GPU资源分布如下：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;与训练相比，执行推理任务的实例具有更高的CPU利用率，因为它们处理源源不断到达的大量数据&lt;/li&gt;
&lt;li&gt;有近75%的实例使用的GPU小于0.1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下图展示了这些模型运行时资源情况：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;CPU资源的使用明显高于其他资源&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;gnn-training&#34;&gt;GNN training&lt;/h4&gt;
&lt;p&gt;图神经网络也是计算密集型任务， CPU的使用率远超GPU，如下图所示：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;在模型训练阶段，需要进行大量的CPU操作&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;reinforcement-learning&#34;&gt;Reinforcement learning&lt;/h4&gt;
&lt;p&gt;加强学习算法通过并行模拟迭代生成一批数据将生成的数据放到GPU上进行训练，以改进学习策略。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;有72%的任务需要超过10个实例来完成，加大调度难度&lt;/li&gt;
&lt;li&gt;但是大多数RL任务对GPU的需求极低&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;部署调度策略&#34;&gt;部署调度策略&lt;/h3&gt;
&lt;h4 id=&#34;reserving-and-packing&#34;&gt;Reserving-and-packing&lt;/h4&gt;
&lt;p&gt;集群中有&lt;strong&gt;意保留高端GPU资源&lt;/strong&gt;，而尽可能将任务打包在一起，共享使用低端GPU资源。&lt;/p&gt;
&lt;p&gt;对于每个任务，调度程序生成一个有序的分配计划序列；每个计划指定了预期的GPU设备，并与尝试超时值相关联。&lt;/p&gt;
&lt;p&gt;对于需要高端GPU的任务，先尝试高端GPU的分配，然后再尝试较低端GPU的分配；对于其他任务，顺序颠倒过来，GPU调度器是在基于局部树的调度系统Fuxi[26,71]上实现的。&lt;/p&gt;
&lt;h4 id=&#34;load-balancing&#34;&gt;Load-balancing&lt;/h4&gt;
&lt;p&gt;在Reserving-and-packing的前提下，调度器还会优先将实例调度到分配率较低的机器上，分配率衡量为已分配的CPU、内存和GPU的加权总和，这些资源按机器的容量进行标准化。&lt;/p&gt;
&lt;h4 id=&#34;benefits&#34;&gt;Benefits&lt;/h4&gt;
&lt;p&gt;具体对应两种调度方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简单地使用渐进式填充的负载平衡机器(总是将任务的实例调度到利用率最低的节点)&lt;/li&gt;
&lt;li&gt;不考虑负载均衡，只执行Reserving-and-packing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下图展示了这两种策略的实际表现：&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;请注意，任务的排队延迟也包括在它的组调度实例的排队延迟&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在这两个策略下，超过90%的实例和任务会立即启动&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与负载均衡算法相比，Reserving-and-packing算法将平均任务排队率降低了45%，主要原因是尾部延迟的显著缩短超过10000秒&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;进一步比较了业务关键型任务和请求V100的实例的排队延迟，在两种策略下，GPU的平均任务排队延迟减少了68%&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;其他待解决的问题&#34;&gt;其他待解决的问题&lt;/h3&gt;
&lt;h4 id=&#34;mismatch-between-machine-specs-and-instance-requests&#34;&gt;Mismatch between machine specs and instance requests&lt;/h4&gt;
&lt;p&gt;该问题带来的影响如下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220417191055419.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220417191055419&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这直接导致：相较于高端机器来说，低端机器明显更拥挤，它们的资源使用率也高于高端机器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;overcrowded-weak-gpu-machines&#34;&gt;Overcrowded weak-GPU machines&lt;/h4&gt;
&lt;h4 id=&#34;imbalanced-load-in-high-end-machines&#34;&gt;Imbalanced load in high-end machines&lt;/h4&gt;
&lt;h4 id=&#34;cpu-can-be-the-bottleneck&#34;&gt;CPU can be the bottleneck&lt;/h4&gt;
</description>
        </item>
        
    </channel>
</rss>
