<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Kubernetes on Li Duo</title>
        <link>https://lizonglingo.github.io/tags/kubernetes/</link>
        <description>Recent content in Kubernetes on Li Duo</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-CN</language>
        <lastBuildDate>Sun, 24 Apr 2022 15:17:07 +0800</lastBuildDate><atom:link href="https://lizonglingo.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>RunWild: Resource Management System with Generalized Modeling for Microservices on Cloud</title>
        <link>https://lizonglingo.github.io/p/runwild-resource-management-system-with-generalized-modeling-for-microservices-on-cloud/</link>
        <pubDate>Sun, 24 Apr 2022 15:17:07 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/runwild-resource-management-system-with-generalized-modeling-for-microservices-on-cloud/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;来源：IEEE CLOUD&#39;21&lt;/p&gt;
&lt;p&gt;作者：IBM&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;star摘要&#34;&gt;:star:摘要&lt;/h2&gt;
&lt;h3 id=&#34;问题背景&#34;&gt;问题背景&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;微服务内部通信的复杂性，必须考虑资源利用、调度策略和请求均衡之间的平衡，以防止跨微服务级联的服务质量下降。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;文章做了什么&#34;&gt;文章做了什么&lt;/h3&gt;
&lt;p&gt;提出资源管理系统&lt;strong&gt;RunWild&lt;/strong&gt;，可以控制所有节点涉及到的微服务管理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;扩缩容&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动&lt;/strong&gt;的根据&lt;strong&gt;指定性能表现&lt;/strong&gt;的&lt;strong&gt;负载和性能平衡优化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;统一的&lt;strong&gt;持续部署方案&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;着重强调了&lt;strong&gt;协同metrics感知&lt;/strong&gt;在&lt;strong&gt;预测资源使用和制定部署计划&lt;/strong&gt;中的重要性。&lt;/p&gt;
&lt;p&gt;在IBM云进行实验，&lt;strong&gt;以K8s的自动调度为基线&lt;/strong&gt;，减少P90响应时间11%，增加10%的吞吐率，降低30%的资源使用。&lt;/p&gt;
&lt;h3 id=&#34;贡献&#34;&gt;贡献&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;扩展的部署框架：&lt;strong&gt;适用于K8s的调度框架&lt;/strong&gt;，用来在资源分配、部署、和运行时来控制部署机制；&lt;/li&gt;
&lt;li&gt;通用的建模方法：综合考虑微服务特性、节点的相对独立性、工作负载和全局协同节点状态感知，通过结合聚类和回归技术预测资源使用；&lt;/li&gt;
&lt;li&gt;微服务间交互指标：一个称为内聚的指标反映了在同一个&lt;strong&gt;节点上放置高度相互通信的微服务的优势&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;通过&lt;strong&gt;Service Mesh&lt;/strong&gt;对运行时工作负载进行分区：利用服务网格操作流量路由，用其控制能力来划分工作负载以匹配资源的分配。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;star现有技术存在的问题&#34;&gt;:star:现有技术存在的问题&lt;/h2&gt;
&lt;h3 id=&#34;水平伸缩&#34;&gt;水平伸缩&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;超过某个阈值时，实例的增多与性能表现的增长不匹配，正如收益递减定律所解释的那样；&lt;/li&gt;
&lt;li&gt;资源过度分配并不会显著增加性能表现；&lt;/li&gt;
&lt;li&gt;而资源不足会导致性能下降或者致命错误。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;垂直伸缩&#34;&gt;垂直伸缩&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;K8s虽然可以支持HPA和VPA，但是不能一起工作，同时进行HPA和VPA难免会造成干扰。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，复杂的资源依赖，全局的资源调度策略使得伸缩方案受多方面影响。&lt;/p&gt;
&lt;p&gt;文章的动机是&lt;strong&gt;识别、描述和管理所有因素和维度&lt;/strong&gt;，以实现&lt;strong&gt;统一的部署解决方案&lt;/strong&gt;，而不是运行相互干扰的机制。&lt;/p&gt;
&lt;h3 id=&#34;部署的三个角度&#34;&gt;部署的三个角度&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;所部署的服务的实例副本数；&lt;/li&gt;
&lt;li&gt;节点上每个实例所得到的资源；&lt;/li&gt;
&lt;li&gt;每个实例的网络容量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;然后引出下面4个重要的问题：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调度所涉及到的策略、资源和具体情景很复杂，要考虑的东西太多；&lt;/li&gt;
&lt;li&gt;同一节点上部署的服务可能&lt;strong&gt;对资源的争用很敏感&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务之间的通信，亲和性&lt;/strong&gt;等因素会影响到&lt;strong&gt;全局的服务性能表现、响应事件及吞吐量&lt;/strong&gt;，最好的方式是使部署的微服务&lt;strong&gt;减少跨节点的通信&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;如何将&lt;strong&gt;请求负载均衡到不同实例以带来更好的网络表现&lt;/strong&gt;，虽然Service Mesh能够实现负载的分发，但是难以解决上述问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章列举了一些其他文章做的工作，并对比这些工作解决了上述4个问题中的哪些：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220423213910376.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220423213910376&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;starrunwild&#34;&gt;:star:RunWild&lt;/h2&gt;
&lt;p&gt;RunWild主要解决：决定实例数量，决定在哪个节点放置实例，如何对工作负载进行分区，如何根据众多资源类型和情景优化部署？&lt;/p&gt;
&lt;p&gt;涉及到的技术有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用循环执行的监控-分析系统对部署进行分析：根据监控和分析环节，设计一个资源使用模型，来预测资源使用。利用automated AI技术获得优化的回归模型来预测资源使用，同时考虑消息请求和节点上资源竞争产生的扰动因素。&lt;/li&gt;
&lt;li&gt;根据分析制定部署计划：定义一个聚合指标，表示微服务间(通信)的联系程度。部署计划应用于所有机器，包括水平部署、资源分配、放置调度和负载均衡。&lt;/li&gt;
&lt;li&gt;执行部署：利用Service Mesh提供的运行时链路控制，通过标签动态将工作负载进行分区。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;系统架构&#34;&gt;系统架构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220424132723898.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220424132723898&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;management-module&#34;&gt;Management Module&lt;/h4&gt;
&lt;p&gt;接收用户提交的部署细节信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Specification Handler：管理输入的部署文件并自动化处理；&lt;/li&gt;
&lt;li&gt;Reconcile Timer：计算并触发每个输入规范的部署自动化过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;computation-module&#34;&gt;Computation Module&lt;/h4&gt;
&lt;p&gt;在全周期中给出部署的解决方案。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor：监控每个实例的资源使用情况和工作负载，文章将工作负载理解为请求的数量；&lt;/li&gt;
&lt;li&gt;Modeler：用来预测资源使用的模型；&lt;/li&gt;
&lt;li&gt;Planner：计算部署计划，包括实例数量、节点放置策略、资源预留、工作负载分区。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;execution-module&#34;&gt;Execution Module&lt;/h4&gt;
&lt;p&gt;用来执行计算的部署结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scaler：更新实例数量；&lt;/li&gt;
&lt;li&gt;Scheduler：将容器放置到计划的节点；&lt;/li&gt;
&lt;li&gt;Partitioner：配置链路控制机制，对工作负载进行计划的分区。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;系统实现&#34;&gt;系统实现&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220424140304736.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220424140304736&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;star实验测试&#34;&gt;:star:实验测试&lt;/h2&gt;
&lt;p&gt;集群包含8个节点，每个节点4vCPU，16GB内存，并部署了Istio和Prometheus。&lt;/p&gt;
&lt;p&gt;部署70个微服务，600个容器实例，收集了3天的数据。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>IEEE CLOUD 21 云上资源管理相关合辑</title>
        <link>https://lizonglingo.github.io/p/ieee-cloud-21-%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3%E5%90%88%E8%BE%91/</link>
        <pubDate>Thu, 21 Apr 2022 14:33:47 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/ieee-cloud-21-%E4%BA%91%E4%B8%8A%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3%E5%90%88%E8%BE%91/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;本篇整理自IEEE CLOUD&#39;21会议中的文章，主题为云背景下的资源管理。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;runwild-resource-management-system-withgeneralized-modeling-for-microservices-on-cloud&#34;&gt;RunWild: Resource Management System withGeneralized Modeling for Microservices on Cloud&lt;/h2&gt;
&lt;h3 id=&#34;star摘要&#34;&gt;:star:摘要&lt;/h3&gt;
&lt;h4 id=&#34;问题背景&#34;&gt;问题背景&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;微服务内部通信的复杂性，必须考虑资源利用、调度策略和请求均衡之间的平衡，以防止跨微服务级联的服务质量下降。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;文章做了什么&#34;&gt;文章做了什么&lt;/h4&gt;
&lt;p&gt;提出资源管理系统&lt;strong&gt;RunWild&lt;/strong&gt;，可以控制所有节点涉及到的微服务管理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;扩缩容&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;调度&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自动&lt;/strong&gt;的根据&lt;strong&gt;指定性能表现&lt;/strong&gt;的&lt;strong&gt;负载和性能平衡优化&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;统一的&lt;strong&gt;持续部署方案&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;着重强调了&lt;strong&gt;协同metrics感知&lt;/strong&gt;在&lt;strong&gt;预测资源使用和制定部署计划&lt;/strong&gt;中的重要性。&lt;/p&gt;
&lt;p&gt;在IBM云进行实验，&lt;strong&gt;以K8s的自动调度为基线&lt;/strong&gt;，减少P90响应时间11%，增加10%的吞吐率，降低30%的资源使用。&lt;/p&gt;
&lt;h4 id=&#34;贡献&#34;&gt;贡献&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;扩展的部署框架：&lt;strong&gt;适用于K8s的调度框架&lt;/strong&gt;，用来在资源分配、部署、和运行时来控制部署机制；&lt;/li&gt;
&lt;li&gt;通用的建模方法：综合考虑微服务特性、节点的相对独立性、工作负载和全局协同节点状态感知，通过结合聚类和回归技术预测资源使用；&lt;/li&gt;
&lt;li&gt;微服务间交互指标：一个称为内聚的指标反映了在同一个&lt;strong&gt;节点上放置高度相互通信的微服务的优势&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;通过&lt;strong&gt;Service Mesh&lt;/strong&gt;对运行时工作负载进行分区：利用服务网格操作流量路由，用其控制能力来划分工作负载以匹配资源的分配。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;star现有技术存在的问题&#34;&gt;:star:现有技术存在的问题&lt;/h3&gt;
&lt;h4 id=&#34;水平伸缩&#34;&gt;水平伸缩&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;超过某个阈值时，实例的增多与性能表现的增长不匹配，正如收益递减定律所解释的那样；&lt;/li&gt;
&lt;li&gt;资源过度分配并不会显著增加性能表现；&lt;/li&gt;
&lt;li&gt;而资源不足会导致性能下降或者致命错误。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;垂直伸缩&#34;&gt;垂直伸缩&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;K8s虽然可以支持HPA和VPA，但是不能一起工作，同时进行HPA和VPA难免会造成干扰。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此外，复杂的资源依赖，全局的资源调度策略使得伸缩方案受多方面影响。&lt;/p&gt;
&lt;p&gt;文章的动机是&lt;strong&gt;识别、描述和管理所有因素和维度&lt;/strong&gt;，以实现&lt;strong&gt;统一的部署解决方案&lt;/strong&gt;，而不是运行相互干扰的机制。&lt;/p&gt;
&lt;h4 id=&#34;部署的三个角度&#34;&gt;部署的三个角度&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;所部署的服务的实例副本数；&lt;/li&gt;
&lt;li&gt;节点上每个实例所得到的资源；&lt;/li&gt;
&lt;li&gt;每个实例的网络容量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后引出下面4个重要的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;调度所涉及到的策略、资源和具体情景很复杂，要考虑的东西太多；&lt;/li&gt;
&lt;li&gt;同一节点上部署的服务可能&lt;strong&gt;对资源的争用很敏感&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;微服务之间的通信，亲和性&lt;/strong&gt;等因素会影响到&lt;strong&gt;全局的服务性能表现、响应事件及吞吐量&lt;/strong&gt;，最好的方式是使部署的微服务&lt;strong&gt;减少跨节点的通信&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;如何将&lt;strong&gt;请求负载均衡到不同实例以带来更好的网络表现&lt;/strong&gt;，虽然Service Mesh能够实现负载的分发，但是难以解决上述问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;文章列举了一些其他文章做的工作，并对比这些工作解决了上述4个问题中的哪些：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220423213910376.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220423213910376&#34;
	
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该文会精读，请关注最新的文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;fast-and-efficient-performance-tuning-of-microservices&#34;&gt;Fast and Efficient Performance Tuning of Microservices&lt;/h2&gt;
&lt;h3 id=&#34;star摘要-1&#34;&gt;:star:摘要&lt;/h3&gt;
&lt;p&gt;针对使用&lt;strong&gt;容器部署的微服务架构应用&lt;/strong&gt;，&lt;strong&gt;以Kubernetes、Docker Swarm容器管理平台为依托&lt;/strong&gt;。在应用正式部署上线之前，也就是在&lt;strong&gt;pre-deployment&lt;/strong&gt;阶段，&lt;strong&gt;迭代的根据资源使用相关指标&lt;/strong&gt;，结合&lt;strong&gt;类多目标优化算法(文章称为heuristic optimization algorithm)&lt;strong&gt;对&lt;/strong&gt;资源分配进行调优&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;star系统架构&#34;&gt;:star:系统架构&lt;/h3&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ol&gt;
&lt;li&gt;将应用部署到云平台；&lt;/li&gt;
&lt;li&gt;进行负载注入；&lt;/li&gt;
&lt;li&gt;基于Jaeger的监控系统开始进行性能测试和追踪(对每个微服务)，收集数据，如响应时间和资源的使用量；&lt;/li&gt;
&lt;li&gt;通过Jaeger解析服务调用序列；&lt;/li&gt;
&lt;li&gt;由Tuning Agent参照服务序列信息、不同类别请求的响应时间和平均资源使用进行调优；&lt;/li&gt;
&lt;li&gt;Tuning Agent预估每个微服务的新的CPU配额信息；&lt;/li&gt;
&lt;li&gt;将这些信息存储到Tuning数据库中；&lt;/li&gt;
&lt;li&gt;编排器根据这些信息对服务进行迭代部署。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;star测量优化的依赖指标&#34;&gt;:star:测量、优化的依赖指标&lt;/h3&gt;
&lt;p&gt;需要对服务进行&lt;strong&gt;请求的注入&lt;/strong&gt;来进行测量，主要指标是&lt;strong&gt;服务响应时间&lt;/strong&gt;。涉及到&lt;strong&gt;链路追踪、性能监控&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id=&#34;star调优模型抽象&#34;&gt;:star:调优模型抽象&lt;/h3&gt;
&lt;h4 id=&#34;小背景前提和假设&#34;&gt;小背景、前提和假设&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用需要一些特定的工作负载$W$&lt;/strong&gt;，这些工作负载发生在特定的情境，例如在线商城的Black Friday。因此，调优过程可以对其他感兴趣的工作负载重放，从而产生一系列特定于工作负载的配置，可以在部署应用程序时适当地使用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本文重点关注CPU资源的限制&lt;/strong&gt;，但是该模型可以拓展到其他资源。&lt;/li&gt;
&lt;li&gt;应用包含**$K$个微服务**，每个微服务运行在自己的container中。&lt;/li&gt;
&lt;li&gt;每个应用支持**$C$种不同的请求类别**。&lt;/li&gt;
&lt;li&gt;每个请求类别**$c$关联到不同的响应时间$T_c$**。&lt;/li&gt;
&lt;li&gt;每类请求**$c$涉及到一个微服务调用序列$S_c$**。&lt;/li&gt;
&lt;li&gt;因此这个序列中每个&lt;strong&gt;微服务$k$都涉及到一个CPU需求&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;主机上对应的&lt;strong&gt;服务$k$所需的CPU配额表示为$\alpha_k$&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;资源分配问题抽象&#34;&gt;资源分配问题抽象&lt;/h4&gt;
&lt;p&gt;问题可以抽象为：在&lt;strong&gt;满足响应时间的需求下，求解对每个微服务CPU配额的最小值&lt;/strong&gt;。&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;ol&gt;
&lt;li&gt;目标为最小化CPU配额；&lt;/li&gt;
&lt;li&gt;需要满足前提条件，即：资源配额能够使某类请求的响应时间$R_c$小于等于目标值$T_c$；&lt;/li&gt;
&lt;li&gt;其中响应时间$R_c$是工作负载$W$和对$K$个服务CPU配额的函数；&lt;/li&gt;
&lt;li&gt;最后限制CPU需求总额是有限的。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;star应用案例及实验&#34;&gt;:star:应用案例及实验&lt;/h3&gt;
&lt;p&gt;使用的微服务案例&lt;strong&gt;Bookstore&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220419153253242.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220419153253242&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;star我的问题&#34;&gt;:star:我的问题&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;工作负载的模拟具体如何实现？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;有哪些开源微服务应用真正可用又具有一定的代表性？&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;skynet-performance-driven-resource-management-for-dynamic-workloads&#34;&gt;Skynet: Performance-driven Resource Management for Dynamic Workloads&lt;/h2&gt;
&lt;h3 id=&#34;star摘要-2&#34;&gt;:star:摘要&lt;/h3&gt;
&lt;h4 id=&#34;问题背景和主要矛盾&#34;&gt;问题背景和主要矛盾&lt;/h4&gt;
&lt;p&gt;云环境下，资源利用率和应用的性能表现之间的矛盾。&lt;/p&gt;
&lt;h4 id=&#34;难点&#34;&gt;难点&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;用户常会&lt;strong&gt;分配过多的资源&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;应用的&lt;strong&gt;多样性和动态性&lt;/strong&gt;，&lt;strong&gt;工作负载的动态性及难以预测性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;性能表现取决于&lt;strong&gt;多种不同资源&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;文章做了什么怎么做的&#34;&gt;文章做了什么，怎么做的&lt;/h4&gt;
&lt;p&gt;提出Skynet，针对上述三个难点，可以自动对云资源进行管理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;评估资源需求&lt;strong&gt;依赖的指标&lt;/strong&gt;：Skynet使用performance level objectives(PLOs)准确捕捉用户对所需性能的意图，将用户从资源分配循环中解放。Skynet&lt;strong&gt;通过目标PLO去预估资源需求&lt;/strong&gt;，使用Poportional Integral Derivative(PID)控制器对每个应用调整对应的参数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;资源需求&lt;strong&gt;计算、分配、调度方法&lt;/strong&gt;：为捕获每个应用对不同资源依赖，&lt;strong&gt;Skynet扩展了传统的一维PID控制器&lt;/strong&gt;(传统的单输入单输出)，实现对CPU、内存、I/O和网络吞吐的预估。Skynet建立一个动态模型，对于每个应用，将目标PLOs映射到资源，同时考虑多种资源和变化的输入负载。事实上，Skynet处于一个&lt;strong&gt;动态循环控制&lt;/strong&gt;来预估资源。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;实现和评估&lt;/strong&gt;：在&lt;strong&gt;kubernetes中将skynetas实现为端到端的定制调度程序&lt;/strong&gt;，并在5个节点的私有集群和60个裸金属服务器AWS上使用真实的工作负载对其进行评估。以K8s为基线，PLO违规降低7.4倍，资源利用提高两倍。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;star系统架构-1&#34;&gt;:star:系统架构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220421114835655.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220421114835655&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;用户可以指定PLOs，明确对吞吐量、延迟、处理时间等指标的需求。Skynet根据这些PLOs，使用PID[41]预估每个应用的资源需求量。动态的将PLO映射到资源需求，这样一来可以让Skynet适应变化的工作负载和每个应用不同的生命阶段。&lt;/p&gt;
&lt;h4 id=&#34;示例&#34;&gt;示例&lt;/h4&gt;
&lt;p&gt;一个web应用PLO为1000请求/秒。Skynet给每个新应用分配一个预定义容器。在执行阶段，Skynet主要使用两个组件：Resource Estimator(RE)和Resource Assigner(RA)，来周期性的调整资源配额以满足PLO：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Skynet周期性监控应用性能指标，如果触发PLO违规，会触发RE。&lt;/li&gt;
&lt;li&gt;RE基于PLO调整PIDs的参数。&lt;/li&gt;
&lt;li&gt;基于目标PLO，RE预估应用新的资源需求。&lt;/li&gt;
&lt;li&gt;当可分配资源满足条件时，RA调整应用容器。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;放置应用以及根据控制器更新应用放置&#34;&gt;放置应用以及根据控制器更新应用放置&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;确定应用资源需求量后，Skynet决定容器的资源限额和放置。具体来说，包括容器打包，节点绑定以及资源配额。其中，容器大小和放置由于需要考虑多种资源的约束，远比打包应用复杂。放置应用的目标是：避免应用间干扰，提高应用性能表现。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当新应用到来时，Skynet进行扫描，查看是否有某个服务节点可以单独满足应用的资源需求，如果不存在这样的服务节点，就迭代执行下列步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;增加一个容器的数量；&lt;/li&gt;
&lt;li&gt;在容器之间平均分配资源；&lt;/li&gt;
&lt;li&gt;找到能够满足容器需求，并且负载最高的服务节点；&lt;/li&gt;
&lt;li&gt;如果没有，循环执行上述步骤。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调整应用资源配额。每次请求改变资源需求时，有三种可能：(理解的有些别扭？)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;资源不够。该情况下，Skynet决定有没有现存的容器可以移除。然后基于节点负载对节点进行排序，移除额外的容器。&lt;/li&gt;
&lt;li&gt;节点上的可用资源早已被分配给应用。Skynet在容器之间平均增加应用程序的资源，以匹配新的请求。&lt;/li&gt;
&lt;li&gt;可用资源分布在不同的服务节点上。Skynet以放置新应用的思路放置新的容器。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的说，就是处理，&lt;strong&gt;容器应该放置在哪个节点上的问题&lt;/strong&gt;。算法思路如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220421143003087.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220421143003087&#34;
	
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;在kubernetes上的实现&#34;&gt;在Kubernetes上的实现&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;https://picgo-lzl.oss-cn-beijing.aliyuncs.com/image-20220421132506202.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20220421132506202&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;使用Golang实现自定义调度器。使用Prometheus进行监控。代码开源[11]。&lt;/p&gt;
&lt;h3 id=&#34;star我的问题-1&#34;&gt;:star:我的问题&lt;/h3&gt;
&lt;h4 id=&#34;关于pid控制理论的补充&#34;&gt;关于PID控制理论的补充&lt;/h4&gt;
&lt;p&gt;已经不止一次在论文中看到使用PID来调整资源分配了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/39573490&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/39573490&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;如果要使用pid算法再细读&#34;&gt;如果要使用PID算法，再细读&lt;/h4&gt;
&lt;p&gt;获得监控数据后具体怎处理？&lt;/p&gt;
&lt;p&gt;分配资源的具体方法？&lt;/p&gt;
&lt;h2 id=&#34;konveyor-move2kube-automatedreplatforming-of-applications-to-kubernetes&#34;&gt;Konveyor Move2Kube: AutomatedReplatforming of Applications to Kubernetes&lt;/h2&gt;
&lt;h3 id=&#34;star摘要-3&#34;&gt;:star:摘要&lt;/h3&gt;
&lt;p&gt;文章提出Move2Kube，一个再部署框架，能够自动调整部署细节，并通过部署pipeline&lt;strong&gt;将非Kubernetes平台部署的应用转移到Kubernetes平台上&lt;/strong&gt;，同时最小限度修改应用架构和实现。&lt;/p&gt;
&lt;p&gt;此外，文章提出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个最小化的中间表示，不同的应用部署构建都可以转化到这个中间表示上来。&lt;/li&gt;
&lt;li&gt;一个扩展框架，用于添加对新的部署源平台和目标中间件的支持，同时允许定制化。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Move2Kube已经开源：https://move2kube.konveyor.io/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;要解决什么问题&#34;&gt;要解决什么问题&lt;/h4&gt;
&lt;p&gt;在不是K8s平台部署的应用迁移到K8s平台上，同时应该最小限度的修改原系统的实现和软件架构。&lt;/p&gt;
&lt;h4 id=&#34;挑战难点在哪里&#34;&gt;挑战、难点在哪里&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;应用规模：企业级应用往往有上千个组件，人工迁移费时费力；&lt;/li&gt;
&lt;li&gt;应用异构：多样的部署平台，多样的应用架构和种类；&lt;/li&gt;
&lt;li&gt;不同的代码源、组件仓库：代码源或者使用的组件分布在不同的仓库中，很难将其组织到一起，如何分布的数千个目录中找到正确的文件很有挑战；&lt;/li&gt;
&lt;li&gt;容器化挑战：将应用容器化时，对于优化配置和分层安全很有必要，需要对容器内部、镜像技术和应用配置有深入的理解；&lt;/li&gt;
&lt;li&gt;目标平台映射：找到正确的不同平台的配置映射关系是困难的，例如如何选择从简单的K8s service转换到Istio的配置中；&lt;/li&gt;
&lt;li&gt;应用的最佳实践：K8s有最佳实践[6]，如何确保迁移使用K8s的最佳实践；&lt;/li&gt;
&lt;li&gt;定制化的需求和有效的Day 2 Operation：针对不同应用和需求定制化的配置以适应平台特性需要一定的经验和时间，同时需要考虑Day 2 Operation。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;关于什么是Day 2 Operation：https://jimmysong.io/blog/what-is-day-2-operation/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;move2kube&#34;&gt;Move2Kube&lt;/h4&gt;
&lt;p&gt;这个开源框架旨在解决应用迁移到Kubernetes平台过程中出现的上述问题。它提供了标准化的Pipeline，包括&lt;strong&gt;容器化、参数化、配置优化、定制化&lt;/strong&gt;等解决方案，满足面向&lt;strong&gt;特定平台的多源、多服务&lt;/strong&gt;的应用部署迁移。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该文不太属于资源管理方面。&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>SHOWAR: Right-Sizing And Efficient Scheduling of Microservices</title>
        <link>https://lizonglingo.github.io/p/showar-right-sizing-and-efficient-scheduling-of-microservices/</link>
        <pubDate>Tue, 22 Feb 2022 13:45:25 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/showar-right-sizing-and-efficient-scheduling-of-microservices/</guid>
        <description>&lt;blockquote&gt;
&lt;p&gt;来源：ACM SoCC&#39;21&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://acmsocc.org/2021/accepted-papers.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://acmsocc.org/2021/accepted-papers.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;解决了什么问题：提出一个微服务资源调度框架，解决微服务的调度问题，具体来说从&lt;strong&gt;水平扩缩容——增减服务实例&lt;/strong&gt;和&lt;strong&gt;垂直扩缩容——控制每个服务CPU和内存等资源的配额&lt;/strong&gt;两个维度对微服务进行调度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;适用于什么环境：该资源调度框架应用于&lt;strong&gt;使用K8s部署的微服务&lt;/strong&gt;上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实验的实施和结果：&lt;strong&gt;使用多个微服务应用和现实世界中的负载情况进行实验&lt;/strong&gt;，资源利用表现提高22%，用户端到端实验降低20%&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目标和宗旨：找到一个最佳资源分配大小，&lt;strong&gt;保持良好服务质量的同时尽可能提高资源利用率&lt;/strong&gt;，减少资源配额&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;微服务调度存在的问题和挑战&#34;&gt;微服务调度存在的问题和挑战&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;确定微服务应用对资源的需求是个复杂工作，难以预先确定&lt;/li&gt;
&lt;li&gt;如果分配过多的资源会造成集群资源利用率低，增加开销&lt;/li&gt;
&lt;li&gt;分配资源过少则导致服务性能下降甚至服务不可用，带来更严重的问题&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;贡献&#34;&gt;贡献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;垂直和水平扩缩容框架，旨在提高资源分配的效率&lt;/li&gt;
&lt;li&gt;调度亲和性和反亲和性规则，为K8s调度程序生成更好的微服务调度规则，提高调度效率&lt;/li&gt;
&lt;li&gt;实现上述要点并评估&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;设计思路&#34;&gt;设计思路&lt;/h2&gt;
&lt;h3 id=&#34;概述-1&#34;&gt;概述&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;垂直扩缩容：参照&lt;strong&gt;历史资源利用率&lt;/strong&gt;来寻找每个微服务的最佳资源配额，调度的资源是每个服务占用的CPU、RAM、Disk等资源&lt;/li&gt;
&lt;li&gt;水平扩缩容：使用Linux内核线程调度程序队列的指标（如&lt;strong&gt;eBPF runq latency&lt;/strong&gt;）为扩缩容指标，同时利用控制理论的思想，在微服务运行时对实例数量进行控制。并设计了一个&lt;em&gt;proportional-integral-derivative&lt;/em&gt;控制器，利用历史扩缩容操作和当前的运行时状态来做出下一个水平扩缩容决策，并保持服务的稳定，调度的资源是增减服务实例数量&lt;/li&gt;
&lt;li&gt;服务间依赖：同时考虑了&lt;strong&gt;服务间依赖关系&lt;/strong&gt;，优先调度应用中负载压力大的微服务（如某个微服务作为其他微服务的引用）&lt;/li&gt;
&lt;li&gt;服务性能：在找到一个最佳配额后，会协助集群调度微服务以获得更好的端到端性能&lt;/li&gt;
&lt;li&gt;K8s亲和性与反亲和性：通过不同微服务的历史资源使用情况为K8s生成调度规则（如某种微服务和某类资源有正相关性或负相关性）&lt;/li&gt;
&lt;li&gt;调度效率：能够快速适应工作负载变化&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;垂直扩缩容&#34;&gt;垂直扩缩容&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;调度依据指标：实例的历史资源使用情况（CPU、RAM、Disk、Network等）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要达成的效果有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在运行时为服务找到合适的资源需求&lt;/li&gt;
&lt;li&gt;最大限度减少过度配置导致的资源使用松弛（松弛度=资源配额-资源使用量）&lt;/li&gt;
&lt;li&gt;最大限度减少OOM错误和CPU负载过高的情况，保证服务质量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;垂直扩缩容的局限：每个实例的资源占有量最大不会超过虚拟机的资源量，所以某些情况下即使将虚拟机的所有资源都给到实例也难以满足要求，这就需要水平扩缩容&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;水平扩缩容&#34;&gt;水平扩缩容&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;调度依赖指标：eBPF指标数据&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;eBPF：允许在内核级别允许安全和低开销的程序，从内核级别收集准确的事件信息，如CPU调度程序决策事件、内存分配事件和网络堆栈中的数据包事件。已经被广泛用于微服务检测、性能提升、链路追踪、负载均衡、网络监控和安全中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;具体思路&#34;&gt;具体思路&lt;/h3&gt;
&lt;h4 id=&#34;垂直自动扩缩容&#34;&gt;垂直自动扩缩容&lt;/h4&gt;
&lt;p&gt;K8s（Google Autopilot也是类似）通过检测一段时间窗口（几分钟到几天）中的CPU和内存使用量来设置下一个事件窗口中的资源。通过一个&lt;code&gt;margin&lt;/code&gt;和观测到的如P95、P99的百分位值，目的是为资源增加一些宽裕度，尽可能减少OOM错误和CPU不够用的情况发生。&lt;strong&gt;作者认为这还不够节约，存在资源浪费的情况发生&lt;/strong&gt;。$\alpha$为宽限额度，$\pi$是某个测量的百分位数值。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220221103059.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;而SHOWAR使用“three-sigma”经验法则去分配资源&lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;SHOWAR收集持续时间W秒的最后一个窗口的每种资源使用的统计数据，每秒收集一次，用于递归计算该窗口上的资源使用平均值$\mu$和方差$\sigma^2$。&lt;/li&gt;
&lt;li&gt;计算$s=\mu + 3\sigma$，这里$s$就是特定资源的一个估计量&lt;/li&gt;
&lt;li&gt;然后每经过T秒（T &amp;laquo; W）评估资源使用量是否发生了很大的变化（如超过15%），一旦超过预期值就实施资源重新分配。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220221104502.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;作者认为使用百分数值的3-σ法则能在保证服务良好运行的情况下最大限度的减少资源浪费，同时使用一个阈值来决定是否进行资源重新分配操作能在资源使用差异较小时不会过度配置资源。虽然$\mu+3\sigma$和$\pi(1+\alpha)$都有明确的统计解释，但是使用$3\sigma$可以更加准确的看到均值的分布。如果方差非常小，则分布几乎是恒定的，这是关于 Pod 资源使用情况的单独有用信息。然而，在$\pi(1+\alpha)$方法中，当方差非常小时，尾部百分位数不能传达有用的信息。此外，安全宽裕度参数 $\alpha$的选择可能是任意的，如果未正确指定，可能会导致资源利用率低下或更多OOM错误。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;关于P90、P95等百分位数值的补充资料：https://www.cnblogs.com/hunternet/p/14354983.html&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;水平自动扩缩容&#34;&gt;水平自动扩缩容&lt;/h4&gt;
&lt;p&gt;水平自动扩缩容目前存在一些缺陷：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由于水平扩缩容的主要形式是通过增减服务实例数量来调整资源分配，服务实例在遇到负载激变发生资源使用抖动时，可能导致极端过度配置或者配置严重不足。为了解决这种情况，有些自动缩放策略引入冷却期的概念，在最后一次操作之后的一段时间内不进行扩缩容。如果出现瞬时负载峰值过高的情况也会因为处于冷却期而避免不必要的扩容操作。&lt;/li&gt;
&lt;li&gt;系统不会将系统微服务的依赖关系考虑在内，而是单独处理某个微服务。实践表明在不考虑微服务相关性的前提下的资源分配和缩放效率低下，并且不一定有助于应对负载变化和保证服务质量。如下图对某个后端服务在5s时注入高负载，然后经过一段时间，后端的高延迟情况传到了前端，如果考虑微服务间依赖关系，那么仅扩容后端微服务就可以解决这个问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220221112213.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以往通常使用CPU利用率作为缩放指标，力求在所有微服务中保持目标 CPU 利用率。但CPU 利用率并不是自动缩放和资源分配的最有效指标，随着负载的增加，几乎所有微服务的 CPU 利用率都会增加，而上图的前端微服务的尾部延迟并不总是随着 CPU 利用率的增加而增加（主要是由于后端微服务的高延迟导致的）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SHOWAR旨在解决上述问题，使用控制理论基本框架来设计有状态的水平缩放系统，在满足SLO指标下保证服务稳定。&lt;/p&gt;
&lt;p&gt;通过观测值与目标值的差别来控制缩放是不准确的：$e=observation-target$​​，为此作者设计了更复杂的控制器&lt;em&gt;pro-portional–integral–derivative (PID) controller&lt;/em&gt;：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220221113150.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对资源的检测使用我们使用&lt;strong&gt;eBPF Linux&lt;/strong&gt;调度程序&lt;strong&gt;runq 延迟度量&lt;/strong&gt;，它表示线程可运行与获取CPU并运行之间的时间。使用Runq延迟的P95作为目标点。&lt;strong&gt;与CPU利用率不同，高runq延迟与每个单独的微服务的高请求尾延迟高度相关，这表明runq延迟可以用作水平自动缩放的合适指标，以防止请求延迟增加。直观地说，runq延迟优于CPU利用率的原因是它表明应用程序线程如何竞争CPU资源，因此需要更多（或更少）的CPU资源&lt;/strong&gt;。在SHOWAR中，使用者要指定目标runq的延迟值作为配置的一部分。&lt;/p&gt;
&lt;p&gt;水平扩缩容的传递函数很简单，如果runq超出目标值，则系统必须向外扩展并增加副本数量，反正小于目标值则缩减服务实例（这里目标是是一个范围？我是这么认为的）。为了防止执行过多的自动缩放操作以响应 runq 延迟指标中的快速变化和瞬时突发性，作者在目标周围设置了一个可配置的界限$\alpha%$​​（默认为 20%）作为缓冲区并且不执行自动缩放操作。自动缩放的增加或减少量是微服务当前副本数量的可配置 𝛽 百分比（默认为 10%），如果实际缩放副本数小于1则默认是1（我的理解是，如该实例有20个副本，则扩容20×0.1=2个副本，如果是4个副本4×0.1=0.4&amp;lt;1即扩容1个副本）。算法如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220221120450.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;水平自动扩缩容的两种架构：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;One For All：单个控制器负责自动扩展所有微服务类型。在每个自动缩放决策中，所有微服务都会根据所有微服务中当前度量值观察的平均值一次缩放。虽然这种方法受益于 PID 控制器，但它没有考虑微服务的微服务依赖关系图。&lt;/li&gt;
&lt;li&gt;One For Each：控制器负责每个微服务。每个控制器监控其相应微服务的自动缩放指标runq，根据上述算法进行缩放。控制器输出的绝对值被排序，具有最高值（最大扩展需求）的那些被优先考虑。对于相等的控制器输出，我们会考虑微服务的依赖关系图，并将后端服务优先于依赖的前端服务（在图的拓扑排序之后）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;将二者串联&#34;&gt;将二者串联&lt;/h4&gt;
&lt;p&gt;在K8s等部署平台上推荐的方法是：一次部署仅使用一类缩放器（如为每个微服务确定固定的实例数量的前提下，部署垂直缩放器，自动调整每个实例的资源分配量；或是确定好资源分配量后，令服务通过水平缩放器自动进行实例数量的增减）。&lt;/p&gt;
&lt;p&gt;SHOWAR通过允许串联部署这二者。首先，作者将任何垂直自动缩放决策优先于任何水平自动缩放决策。因为，例如在内存自动缩放的情况下，如果 Pod 的内存不足，应用程序会遇到OOM错误并停止执行，而不管其副本数如何，所以水平自动缩放器无法解决OOM问题。因此，在水平自动缩放控制器动作之前，它首先检查共享通道以查看该微服务是否正在进行垂直自动缩放，如果是则不会继续操作。类似地，在垂直Pod自动缩放器动作之前，它会通过共享通道发送消息通知水平自动缩放器，然后执行其操作。&lt;/p&gt;
&lt;p&gt;此外，由于谷歌云平台的 Kubernetes 最佳实践，建议大多数 Pod 不需要超过一个核心，作者根据这个建议将其合并到 SHOWAR 的垂直自动缩放器设计中：如果垂直自动缩放器决定为 Pod 设置多个核心，它会改为通过共享通道向水平自动缩放器发出信号，并且不会继续执行垂直自动缩放操作。即：核心数增加转化为实例数量增加。&lt;/p&gt;
&lt;h4 id=&#34;利用k8s亲和性和反亲和性获取更好的调度性能&#34;&gt;利用K8s亲和性和反亲和性获取更好的调度性能&lt;/h4&gt;
&lt;p&gt;关于K8s的亲和性和反亲和性可以概括为：服务𝑆2与服务𝑆1的亲和性意味着调度程序将始终（或最好）尝试将服务 𝑆1 的 Pod 调度到服务 𝑆2 所在的节点上。类似地，服务𝑆2与服务𝑆1的反亲和性意味着调度程序永远不会（或最好不）这样做。&lt;/p&gt;
&lt;p&gt;SHOWAR监控和使用微服务的历史（即最后（可配置）时间窗口）CPU、内存和网络使用情况，并计算每对微服务使用模式之间的Paerson相关系数来计算相关性：给定两种微服务类型𝑋和𝑌的CPU（或内存或网络I/O）使用分布，𝑋和𝑌之间的相关系数$\rho$​为：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222104614.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;对于两个微服务𝑆1和𝑆2，资源使用模式（例如CPU或内存）的正相关性越高，它们之间对该资源的资源争用就越高。同样，负相关越低，两个服务之间对该资源的争用就越低。这是 SHOWAR 对 CPU、内存和网络 I/O 等计算资源的亲和性和反亲和性规则的简单基础。&lt;/p&gt;
&lt;p&gt;进一步产生亲和性和反亲和性规则，规则生成机制如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CPU和NetWork：如果两个服务s1、s2的CPU和网络是用呈现强负相关（$\rho_{s1s2}\leq-0.8$​​​​​）,则为其生成亲和性规则。&lt;/li&gt;
&lt;li&gt;Memeory：如果任何一对微服务s1和s2在它们的内存使用模式中具有强正相关（例如$\rho_{s1s2}\geq-0.8$​），则SHOWAR 为调度程序生成s1和s2的反亲和性规则。(实际上也是负相关$\Longrightarrow$亲和性)。&lt;/li&gt;
&lt;li&gt;此外，为避免调度冲突，每个微服务在任意时间最多参与一个亲和性或反亲和性规则。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;实现&#34;&gt;实现&lt;/h2&gt;
&lt;h3 id=&#34;系统架构&#34;&gt;系统架构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222110542.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;monitoring-agents&#34;&gt;Monitoring Agents&lt;/h3&gt;
&lt;p&gt;使用Prometheus从节点和容器收集不同的指标。通过集群中的每个节点上启动一个监控代理来收集容器指标，例如CPU使用率、内存使用率、网络带宽使用率等。代理被配置为每秒收集和报告一次指标。Prometheus附带一个时间序列数据库，代理存储收集到的指标。此外，还提供了一种查询语言来查询其他模块使用的时间序列数据库以利用收集的指标。&lt;/p&gt;
&lt;p&gt;此外作者还开发了一个eBPF程序作为监控代理部署在集群中的每个节点上，以收集水平自动缩放器使用的 &lt;em&gt;runq latency&lt;/em&gt;指标。该指标是每个Pod的CPU线程在获取CPU之前所经历的延迟的直方图。程序每 1 秒收集一次&lt;em&gt;runq latency&lt;/em&gt;直方图，并将其存储在Prometheus时间序列数据库中。&lt;/p&gt;
&lt;h3 id=&#34;the-vertical-autoscaler&#34;&gt;The Vertical Autoscaler&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;垂直自动缩放器是一个简单的循环，每分钟发生一次。&lt;/li&gt;
&lt;li&gt;它会在前5分钟的窗口中为每种资源类型r（CPU和内存）评估$s_r = \mu_r + 3*\sigma_r$，如果s的值变化超过 15%，它会更新服务的资源需求s。&lt;/li&gt;
&lt;li&gt;触发垂直自动缩放器的另一个条件是微服务报告 OOM 错误。&lt;/li&gt;
&lt;li&gt;在应用微服务的新资源需求之前，垂直自动缩放器通过共享通道向水平自动缩放器发送一条消息，以不继续任何水平自动缩放操作，因为&lt;strong&gt;垂直自动缩放操作优先于水平自动缩放&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;如果微服务的CPU数量超过一个CPU核心，垂直自动缩放器也不会继续执行微服务的自动缩放操作，在这种情况下，它会通过另一个共享通道向水平自动缩放器以触发水平自动缩放操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-horizontal-autoscaler&#34;&gt;The Horizontal Autoscaler&lt;/h3&gt;
&lt;p&gt;对于给定的目标&lt;em&gt;runq latency&lt;/em&gt;，它对该微服务执行水平自动缩放操作，使其始终具有目标值的&lt;em&gt;runq latency&lt;/em&gt;。控制器每1分钟决定eBPF程序收集60个度量直方图实例（每秒1个）。对于每个直方图，选择第 95个百分位数，控制器使用这60个数据点的平均值作为其当前观察值（也称为测量值）来执行其控制动作。每个水平扩展操作添加或删除至少1个或可配置百分比（默认为 10%）的微服务当前副本数，分别用于扩缩容。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222112427.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;PID控制参数的初始值取为$k_P=k_I=k_D=1/3$​​（每个参数限制为∈[0,10]，这几个参数会影响控制器的速度、稳定性和准确性）。这些参数的增量变化是 10%（我们通过实验发现 10% 可以提供非常好的性能）。控制器输出的波动是进行此类更改的基础，使用之前的N =10个样本进行测量。此外，控制器的“速度”被测量为达到区间[target(1 − 𝛼), target(1 + 𝛼)] 所需的迭代次数，因为 𝛼 =10%。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;增加$k_P$​会导致控制器速度执行增加（以达到稳定状态），同时过高的值可能引发不稳定性。&lt;/li&gt;
&lt;li&gt;增加$k_I$​也会增加控制器的速度并可能导致不稳定，但增加它会降低控制器的噪声（变化和波动）和稳态误差。&lt;/li&gt;
&lt;li&gt;增加$k_D$​会增加控制器的速度（达到稳态）以及不稳定的可能性，同时会显著放大控制器的噪声。&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;控制器从系数的相等值开始。&lt;/li&gt;
&lt;li&gt;随后这些系数基于监控的工作负载性能和控制器状态进行自适应和增量自调整。&lt;/li&gt;
&lt;li&gt;如果当前指标值（尤其是runq延迟）远离目标指标值，则在每次迭代中增加$k_P$和$k_I$，以提高稳定性以及达到目标指标值的速度。&lt;/li&gt;
&lt;li&gt;此外，如果观察到度量值的波动（在控制器中称为噪声），$k_D$​会逐渐减小以减少工作负载突发性引入的噪声。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;the-affinity-rule-generator&#34;&gt;The Affinity Rule Generator&lt;/h3&gt;
&lt;p&gt;亲和性规则生成器每5分钟使用一次CPU、内存和网络利用率，这是一个由300个数据点组成的向量（每个数据点是微服务副本的平均值）来计算每个资源类型之间的相关系数一对微服务。为消除弱相关或无相关实例，[−0.8,+0.8]中的任何值都会被丢弃。其他强负相关和强正相关的微服务用于生成亲和性和反亲和性规则。资源使用模式会随着工作负载的变化（也称为工作负载转移）而变化，因此如果在随后的5分钟时间窗口内强烈的负相关或正相关变化超过20%（可配置），SHOWAR会撤销亲和性（或那对微服务的反亲和性）规则。&lt;/p&gt;
&lt;h3 id=&#34;其他要点&#34;&gt;其他要点&lt;/h3&gt;
&lt;p&gt;SHOWAR是作为Kubernetes控制器构建的，对于自动扩缩器和其他类型的控制器具有高度可插入性。此外，SHOWAR使用常用的Kubernetes监控代理（例如Prometheus）和一个自定义的eBPF指标监控代理。因此，与默认的Kubernetes自动缩放器相比，&lt;strong&gt;SHOWAR不会引入任何额外的开销&lt;/strong&gt;。此外，自动缩放器被调度在控制器节点上，并且不与调度在工作节点上的应用程序 Pod 共享资源。&lt;/p&gt;
&lt;h2 id=&#34;实验&#34;&gt;实验&lt;/h2&gt;
&lt;p&gt;在AWS部署K8s集群，用Google Autopilot和K8s默认的调度程序作比较。资源利用率提升22%，延迟降低20%。&lt;/p&gt;
&lt;h3 id=&#34;实验设置&#34;&gt;实验设置&lt;/h3&gt;
&lt;h4 id=&#34;applications&#34;&gt;Applications&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;社交网络应用：包含36个微服务，可以关注他人、撰写帖子、阅读他人帖子并与之互动。&lt;/li&gt;
&lt;li&gt;火车票应用：包含41个微服务的应用程序，允许其用户在线预订门票并进行支付。&lt;/li&gt;
&lt;li&gt;谷歌云平台的线上精品店：由 10 个微服务组成，用户可以通过他们的在线购物车购买在线商品并进行支付。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验将runq延迟的目标值设置为15𝑚𝑠，即 Linux 内核 sysctl_sched_latency[31] 调度程序参数的 2.5𝑥。&lt;/p&gt;
&lt;h4 id=&#34;cluster-setup&#34;&gt;Cluster Setup&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;在AWS上进行的。&lt;/li&gt;
&lt;li&gt;使用 𝑚5.𝑥𝑙𝑎𝑟𝑔𝑒 VM 实例，每个实例具有 4 个 vCPU、16 GB 内存和 0.192 美元/ℎ𝑟 价格。&lt;/li&gt;
&lt;li&gt;运行Ubuntu 18.04 LTS，配置为支持运行eBPF程序。&lt;/li&gt;
&lt;li&gt;除非另有说明，否则集群都是由25个VM实例组成。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;workload-and-load-generation&#34;&gt;Workload and Load Generation&lt;/h4&gt;
&lt;p&gt;我们使用Wikipedia访问跟踪[59]作为我们的主要工作负载。它是用户与Wikipedia网站交互的真实世界轨迹，由流量模式组成，包括&lt;strong&gt;泊松到达时间、短期突发性和昼夜水平变化&lt;/strong&gt;。由于我们正在评估的微服务是面向用户的应用程序，因此工作负载必须反映真实的用户行为。因此，维基百科访问跟踪非常适合我们的评估。我们以分布式方式使用&lt;strong&gt;locust&lt;/strong&gt; [26]作为我们的工作负载生成器。 Locust客户端驻留在与托管应用程序的主集群不同的VM实例上。&lt;/p&gt;
&lt;h4 id=&#34;baselines&#34;&gt;Baselines&lt;/h4&gt;
&lt;p&gt;Kubernetes默认自动缩放器和 Google Autopilot。&lt;/p&gt;
&lt;h3 id=&#34;vertical-autoscaling&#34;&gt;Vertical Autoscaling&lt;/h3&gt;
&lt;p&gt;首先评估 SHOWAR 的垂直自动缩放器（禁用水平自动缩放器）在减少相对内存松弛方面的有效性。&lt;/p&gt;
&lt;p&gt;使用来下图中所示的 Wikipedia 访问跟踪的一小时长的工作负载进行评估。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222120133.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;每5分钟记录一次垂直自动缩放器为每个微服务设置的内存限制以及微服务的实际使用情况，以计算其内存使用松弛（即松弛 = 限制 - 使用）。&lt;/p&gt;
&lt;p&gt;下图描绘了社交应用所有微服务相对内存使用松弛的&lt;strong&gt;累积分布函数&lt;/strong&gt;（the cumulative distribution function，CDF）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222120655.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;可以看出，通过历史资源使用的变化（使用3-σ规则），SHOWAR 的垂直自动缩放器与Autopilot 和 Kubernetes 的垂直自动缩放器相比能够改善内存使用松弛度。特别是，对于95%的服务实例，相对内存使用松弛率小于46%，而 Kubernetes 和 Autopilot 分别为 63% 和 66%。这 20% 的内存使用松弛可用于调度更多的服务实例或在集群中使用更少的 VM 资源，这将明显降低成本（见 5.5 小节）。我们还观察到 Kubernetes 的性能优于 Autopilot，因为它在设置限制方面采用了更激进的方法（使用 P95 × 1.15 的过去使用量与最大值相比）。&lt;/p&gt;
&lt;p&gt;虽然低内存或 CPU 使用松弛可以导致高效且具有成本效益的资源分配，但它可能导致更高的 OOM 率或 CPU 节流，从而降低服务性能。下图显示了实验过程中 OOM 的数量。可以看出，虽然与 Kubernetes 相比，SHOWAR 的 OOM 数量相当，但与 Autopilot 相比，它们在内存扩展方面的激进方法导致了更多的 OOM。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222124148.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;下图则描绘了在实验过程中微服务的平均 CPU 节流（CPU 紧松弛的结果）。当 Pod 的 CPU 使用率超过其分配的 CPU 资源时，容器运行时（使用𝑐𝑔𝑟𝑜𝑢𝑝𝑠）会限制 Pod 的 CPU 份额。可以看出，由于微服务 CPU 使用率的高波动（方差），SHOWAR 的 CPU 节流与基线相当。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222124339.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;根据以上三图的分析，&lt;strong&gt;资源松弛度（也反映了资源使用效率）和系统稳定性之间存在权衡&lt;/strong&gt;。SHOWAR和K8s在带来更好的资源效率的同时回不可避免的导致更多的OOM错误和CPU性能限制。而 Autopilot 会导致更多的松弛和更少的 OOM。&lt;/p&gt;
&lt;p&gt;因此，根据任务目标可调整 SHOWAR 和 Kubernetes 以实现更高的稳定性，但代价是更高的资源使用松弛度。例如，在 SHOWAR 中，可以使用 𝑘𝜎 代替 3𝜎 ，其中 𝑘 &amp;gt;3 为单个 Pod 分配更多资源并减轻 OOM 和 CPU 节流。&lt;/p&gt;
&lt;h3 id=&#34;horizontal-autoscaling&#34;&gt;Horizontal Autoscaling&lt;/h3&gt;
&lt;p&gt;使用垂直扩缩容相同的工作负载来评估水平扩缩容。将 SHOWAR 的 One for Each 和 One for All 设计与 Autopilot 和 Kubernetes 水平自动缩放器进行比较。Autopilot 和 Kubernetes 在水平自动缩放中使用相同的方法。我们将 Autopilot 和 Kubernetes 的目标 CPU 利用率设置为 65%，这是通常的建议。&lt;/p&gt;
&lt;p&gt;下图描绘了在实验过程中社交网络应用程序中微服务副本数量的累积分布函数。我们观察到 SHOWAR 的水平自动缩放器都优于 Autopilot 和 Kubernetes 水平自动缩放器，因为它为大多数微服务分配了更少的副本，这反过来又可以更有效地分配资源并节省成本（见 5.5 小节）。通过为每个微服务定制一个控制器，SHOWAR 的 One for Each 设计也优于 One for All。这是因为在 One for All 设计中，单个控制器尝试使用单个目标 runq 延迟值和跨所有微服务的平均 runq 延迟测量来扩展微服务，这会导致不必要的微服务扩展和高 runq 延迟。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222125146.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;再次强调，SHOWAR 的有效性是由于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自动缩放器的状态控制器&lt;/li&gt;
&lt;li&gt;用于自动缩放决策的更好的代表性指标（即 runq 延迟而不是 CPU 利用率）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们看到了在单个微服务的自动扩展决策中使用更有意义和代表性的指标的效果。特别是，在评估过程中，我们观察到 Kubernetes 和 Autopilot 通常为 nginx 设置 16 个副本，这主要是因为它的 CPU 利用率很高。但&lt;strong&gt;高 CPU 利用率并不总是对应于微服务性能的大幅提升&lt;/strong&gt;。相比之下，SHOWAR 只为这个微服务设置了 10 个副本。另一方面，对于其他几个微服务所依赖的 User 微服务，Kubernetes 和 Autopilot 通常只为其设置 3 个副本。相比之下，SHOWAR 通常为此微服务设置 6 个副本。&lt;/p&gt;
&lt;h3 id=&#34;the-effect-of-affinity-and-anti-affinity-rules&#34;&gt;The Effect of Affinity and Anti-Affinity Rules&lt;/h3&gt;
&lt;p&gt;实验使用不同微服务之间 CPU、内存和网络 I/O 使用率的相关性来评估 SHOWAR 生成的 Pod 亲和性和反亲和性规则的效果。仍使用相同的工作负载并禁用垂直和水平扩缩容控制器，以凸显亲和性和反亲和性生成器的工作效果，以此观察K8s调度器受其的影响。&lt;/p&gt;
&lt;p&gt;同时观测了这如何影响端到端请求延迟，如下图所示。通过为调度程序提供调度提示（使用亲和性和反亲和性），SHOWAR 能够改善用户体验的 P99 延迟。特别是，使用 SHOWAR 生成的亲和和反亲和规则，请求延迟的 P99 为 6600 毫秒，而使用 Kubernetes 默认调度决策为 9000 毫秒。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222125945.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;end-to-end-performance&#34;&gt;End-to-End performance&lt;/h3&gt;
&lt;p&gt;在端到端性能评估这部分使用三个组件协同工作，使用下图的工作负载进行测试。为了适应工作负载，我们将集群的大小增加到 30 个 VM 实例。结果表明，与基线相比，SHOWAR 改进了资源分配和利用率，同时保持性能的稳定。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222131341.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;下图描述了用户在实验的 24 小时内经历的端到端请求延迟的 CDF。可以看出，使用 SHOWAR 的端到端性能与基线相当，并且使用其亲和性和反亲和性规则生成器以及依赖关系感知的水平自动缩放，与 Autopilot 和 Kubernetes 相比SHOWAR 能够将 P99 延迟提高 20% 以上。 Autopilot 和 Kubernetes 在 P99th 延迟方面表现出相似的性能，但是，由于为副本分配了更多内存，Autopilot 通常在较低的尾部优于 Kubernetes。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222131527.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;下图显示了实验过程中集群中的总内存分配（即为微服务副本设置的内存限制总和）。与基线相比，SHOWAR 平均为微服务副本分配的内存更少。特别是，SHOWAR 平均分配了 205 GB，而 Autopilot 和 Kubernetes 分别分配了 264 GB 和 249 GB。主要是因为 SHOWAR 的垂直自动缩放器实现了较低的内存使用松弛度，并且其水平自动缩放器为微服务设置了较少的副本数量。因此，使用 SHOWAR 的总内存分配小于基线。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222131959.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;最后，在下图中，展示了每个实验的归一化集群成本。我们将平均内存分配标准化为集群中一台虚拟机的内存大小（即 16 GB 用于𝑚5.𝑥𝑙𝑎𝑟𝑔𝑒 实例），并将其乘以 24 小时内一台虚拟机的成本（即 $0.192/ℎ𝑜𝑢𝑟）。这是因为，通常虚拟机在公共云上的价格是内存大小的线性函数 [17]。可以看出，与 Autopilot 和 Kubernetes 相比，SHOWAR 将集群总成本效益分别提高了 22% 和 17%。这些改进来自这样一个事实，即与基线相比，SHOWAR 的垂直和水平自动缩放器用&lt;strong&gt;分配更少的计算资源就可以达到相同的性能和服务质量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20220222135039.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;不足和未来工作&#34;&gt;不足和未来工作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SHOWAR是基于历史和现在进行反应式调度的，&lt;strong&gt;缺乏预测能力&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们将 SHOWAR 设计为计算量轻且适应性强，这与使用需要训练且无法应对工作负载转移的机器学习的“黑盒”方法形成对比，例如 [39,55,57]。然而，目前 SHOWAR 的一个主要限制是它对微服务的资源使用是反应性的。因此，&lt;strong&gt;一个合适的探索途径是为 SHOWAR 配备近期工作负载和资源使用预测&lt;/strong&gt;，例如 [18]。结合其当前设计，&lt;strong&gt;预测近期工作负载可以改善 SHOWAR 的资源分配并防止由于自动缩放操作不足而导致性能下降&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;另一个限制是它只关注微服务自动扩展并假设一个固定大小的集群&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解决应用程序自动缩放器请求的资源总量超过可用集群资源总量的场景非常重要&lt;/strong&gt;。虽然集群自动缩放与应用程序自动缩放是正交的，但它们需要协同工作以实现资源分配的整体效率和应用程序的性能要求。因此，需要两个自动扩缩器之间的通信和协调才能向集群添加更多资源。在未来的工作中，我们&lt;strong&gt;计划改进 SHOWAR 的自动缩放器以与现有的集群自动缩放器 [12] 一起使用&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;还不适用于Serverless类型的工作负载&lt;/p&gt;
&lt;p&gt;原因之一是垂直自动缩放不适用，因为&lt;strong&gt;无服务器功能的容器大小是预定义的&lt;/strong&gt;。 SHOWAR 的水平自动缩放器可能面临额外的复杂性，例如，跟踪“休眠”无服务器函数的数量（可以热启动）以及每个函数“过期”的时间（因此需要冷启动延迟）。我们将探索无服务器功能水平扩展的控制理论方法留给未来的工作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计划改进 SHOWAR 的亲和性和反亲和性规则生成器&lt;/p&gt;
&lt;p&gt;目前使用简单的经验资源利用相关系数来确定微服务之间的成对亲和力。例如，我们可以在未来&lt;strong&gt;探索其他统计数据对亲和力的影响&lt;/strong&gt;，例如不同类型资源之间的互相关，并&lt;strong&gt;探索不同类型的调度机制&lt;/strong&gt;，这些调度机制可以直接利用这些“原始”统计信息来提高效率资源利用[19]。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>笔记 &gt; SHOWAR: Right-Sizing And Efficient Scheduling of Microservices</title>
        <link>https://lizonglingo.github.io/p/%E7%AC%94%E8%AE%B0-showar-right-sizing-and-efficient-scheduling-of-microservices/</link>
        <pubDate>Mon, 20 Dec 2021 14:51:34 +0800</pubDate>
        
        <guid>https://lizonglingo.github.io/p/%E7%AC%94%E8%AE%B0-showar-right-sizing-and-efficient-scheduling-of-microservices/</guid>
        <description>&lt;h2 id=&#34;摘要&#34;&gt;摘要&lt;/h2&gt;
&lt;p&gt;提出微服务的面临的一个挑战是为每个微服务找到最佳的分配资源和服务实例的数量。达到保证性能的同时最大限度的提高资源利用率这样一个目标。本文的SHOWAR是一个通过确定服务实例数量（横向扩展）以及每个服务实例的资源如CPU和内存（纵向扩展）来配置资源的框架。&lt;/p&gt;
&lt;p&gt;对于纵向扩展，SHOWAR通过历史资源中的经验方差来寻找最佳资源分配量，保证性能同时减少不必要的资源浪费；对于横向扩展，使用控制理论的基本思想以及内核级性能指标来实施。&lt;/p&gt;
&lt;p&gt;在确定微服务的现有状态后，SHOWAR使用调度程序生成亲和性规则来弥合最佳资源分配和调度之间的差距，实现资源分配和性能提高。&lt;/p&gt;
&lt;p&gt;实验表明，SHOWAR与现有的最先进的自动缩放和调度系统相比，资源分配提高了22%，同时降低了99%的端到端请求延迟20%。&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro&lt;/h2&gt;
&lt;p&gt;本文的SHOWAR是一个用于微服务横向和纵向自动扩展的微服务管理系统，用于Kubernetes编排的微服务系统。&lt;/p&gt;
&lt;p&gt;对于纵向缩放，SHOWAR 依赖&lt;strong&gt;历史资源使用情况的差异&lt;/strong&gt;，找到每个微服务的最佳资源大小，保持良好的性能的同时提高资源利用率。&lt;/p&gt;
&lt;p&gt;对于横向自动缩放，SHOWAR 使用来自 &lt;strong&gt;Linux 内核线程调度程序队列的指标&lt;/strong&gt;（特别是 eBPF 运行时延迟）作为其自动缩放信号，以做出更准确和有意义的自动缩放决策。为了实现这个目标，SHOAWR使用了控制理论的基本思想，基于来自&lt;strong&gt;微服务运行时&lt;/strong&gt;的信号控制每个微服务的副本数量。&lt;/p&gt;
&lt;p&gt;该团队设计了一个比例积分微分proportional–integral–derivative (PID) 控制器作为有状态自动缩放器，它使用历史自动缩放操作和当前运行时测量来做出下一个水平自动缩放决策并保持微服务“稳定”。此外，SHOWAR考虑不同微服务之间的依赖关系，优先考虑被依赖的微服务，以防止不必要的自动缩放操作和低资源利用率。&lt;/p&gt;
&lt;p&gt;除了使用自动缩放器来确定微服务的资源外，SHOWAR还旨在桥接微服务的最佳资源分配和高效调度，&lt;strong&gt;在达成最佳资源分配和高效调度之间取得最佳平衡&lt;/strong&gt;。一旦确定了微服务的最佳大小，SHOWAR就会协助集群调度程序调度微服务以获得更好的端到端性能。为了防止资源争用和管理噪声邻居对微服务性能的影响，SHOWAR使用不同微服务之间历史资源使用情况的估计相关性来为Kubernetes调度程序生成规则。例如，这些规则可能会建议调度程序共同定位（调度亲和性）与某种资源类型具有负相关性的微服务，或者以其他方式分发它们（调度反亲和性）。&lt;/p&gt;
&lt;p&gt;文章通过在AWS公共云上的虚拟机集群部署各种交互式微服务应用程序来评估SHOWAR。将SHOWAR的性能与两种最先进的自动缩放系统进行了比较：Google Autopilot和Kubernetes 默认的自动缩放器。使用实际生产工作负载，结果表明，SHOWAR在有效资源分配和端到端请求延迟的尾部分布方面优于这些参照。SHOWAR 平均将资源分配提高了22%，这可以直接转化为集群相关成本的总节省22%，同时将99%的端到端用户请求延迟降低20%。&lt;/p&gt;
&lt;h2 id=&#34;贡献&#34;&gt;贡献&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;提出一种自动化的纵向扩容和横向扩容框架，达到保证服务性能的前提下提高资源利用率的目标&lt;/li&gt;
&lt;li&gt;提出调度亲和性和反亲和规则，通过生成调度亲和性和反亲和性规则来帮助调度程序更好地放置微服务并提高微服务性能，弥合了适当调整微服务规模以提高资源效率和高效微服务调度之间的差距&lt;/li&gt;
&lt;li&gt;通过实验证明SHOWAR的良好表现&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;前置知识&#34;&gt;前置知识&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20211220134037.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;这一个比较典型的微服务架构示意图，微服务之间的依赖关系错综复杂。&lt;strong&gt;其中一些微服务依赖于其他微服务，SHOWAR使用此依赖关系图信息来做出更好的自动缩放决策&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;除了 CPU 和内存使用数据外，SHOWAR还使用扩展的伯克利数据包过滤 ( Berkeley Packet Filtering - eBPF) [6] 指标数据进行水平自动缩放决策。&lt;strong&gt;eBPF 是最新的Linux内核技术，它支持在内核级别运行安全且低开销的程序，以从内核级别的事件（例如 CPU 调度程序决策事件、内存分配事件和内核网络堆栈中的数据包事件）中收集准确的指标&lt;/strong&gt;。它已被广泛用于微服务可观察性，用于性能改进、分析和跟踪、负载平衡、网络监控和安全等广泛目的。&lt;/p&gt;
&lt;h2 id=&#34;showar&#34;&gt;SHOWAR&lt;/h2&gt;
&lt;h3 id=&#34;系统架构&#34;&gt;系统架构&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lizonglin313/MyPicGo/master/20211220141140.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
	
&gt;&lt;/p&gt;
&lt;p&gt;SHOWAR使用每个节点上的相应代理来收集资源使用日志以及 eBPF 指标，然后聚合到时间序列数据库中。&lt;/p&gt;
&lt;p&gt;SHOWAR使用收集到的指标通过分别与Kubernetes API服务器及其调度程序通信来做出自动缩放决策以及调度亲和性和反亲和性规则。&lt;/p&gt;
&lt;h3 id=&#34;系统实现&#34;&gt;系统实现&lt;/h3&gt;
&lt;p&gt;SHOWAR 作为服务部署在控制器节点并与kubernetes API服务器及其调度程序交互以进行自动缩放操作以及为微服务应用生成的亲和性和反亲和性规则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;监控代理Monitoring Agents&lt;/p&gt;
&lt;p&gt;监控和日志数据是任何应用程序部署最重要的部分。监控数据用于可观察性、健康检查和自动缩放。本文使用最先进的监控和指标收集工具Prometheus从节点和容器收集不同的指标。Prometheus在集群中的每个节点上启动一个监控代理来收集容器指标，例如 CPU 使用率、内存使用率、网络带宽使用率等。代理被配置为每秒收集和报告指标（一秒是Prometheus 代理可以收集指标的最短时间。为了获得尽可能多的数据点，我们每秒钟收集一次数据）。Prometheus 带有一个时间序列数据库，代理存储收集的指标。此外，提供查询语言来查询其他模块使用的时间序列数据库以利用收集的指标。&lt;/p&gt;
&lt;p&gt;除Prometheus外，文章还开发了一个eBPF程序，该程序作为监控代理部署在集群中的每个节点上，以收集横向自动缩放器使用的 𝑟𝑢𝑛𝑞 𝑙𝑎𝑡𝑒𝑛𝑐𝑦 指标。该指标是每个pod中的CPU线程在获取CPU之前所经历的延迟直方图。程序每1秒收集一张𝑟𝑢𝑛𝑞 𝑙𝑎𝑡𝑒𝑛𝑐𝑖𝑒𝑠的直方图并存储在Prometheus时间序列数据库中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;纵向缩放器The Vertical Autoscaler&lt;/p&gt;
&lt;p&gt;这一个简单的循环，每分钟进行一轮。在前 5 分钟的窗口内为每种资源类型 𝑟（CPU 和内存）计算 𝑠𝑟 =𝜇𝑟 +3∗𝜎𝑟，如果 𝑠 的值变化超过 15%，它会更新服务的资源需求为𝑠。&lt;/p&gt;
&lt;p&gt;触发缩放器的另一个条件是微服务报告 OOM 错误时。在应用微服务的新资源需求之前，纵向自动缩放器通过共享通道向横向自动缩放器发送消息，不让其进行任何横向自动缩放操作，因为纵向自动缩放操作的优先级高于水平自动缩放。&lt;/p&gt;
&lt;p&gt;如果该微服务的 CPU 数量超过一个 CPU 内核（即 𝑠𝐶𝑃𝑈 &amp;gt;1000𝑚），纵向自动缩放器也不会对微服务进行自动缩放操作，在这种情况下，它会通过另一个共享通道发送消息到横向自动缩放器触发横向自动缩放操作。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;横向缩放器The Horizontal Autoscaler&lt;/p&gt;
&lt;p&gt;横向自动缩放器的核心是一个 PID 控制器，旨在保持每个微服务稳定。对于给定的目标 𝑟𝑢𝑛𝑞 𝑙𝑎𝑡𝑒𝑛𝑐𝑦，它对该微服务执行水平自动缩放操作，使其始终具有𝑟𝑢𝑛𝑞𝑟𝑢𝑛𝑞𝑙控制器每 1 分钟做出决定，eBPF 程序收集 60 个度量直方图实例（每秒 1 个）。对于每个直方图，选择第 95 个百分位数，控制器使用这 60 个数据点的平均值作为其当前观察（也称为测量）来执行其控制操作。每个水平扩展操作添加或删除至少 1 个或可配置百分比（默认为 10%）的微服务当前副本数，分别用于扩展和缩减。PID 控制参数的初始值取为 𝑘𝑃 =𝑘𝐼 =𝑘𝐷 =1/3（每个参数约束为 ∈ [0,10]）。这些参数的增量变化是 10%（我们通过实验发现 10% 的性能非常好）。控制器输出的波动是进行此类更改的基础，使用之前的 𝑁 = 10 个样本进行测量。此外，控制器的“速度”被测量为达到区间 [target(1 −𝛼),target(1 + 𝛼)] 所需的迭代次数，因为 𝛼 =10%。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;亲和规则生成器The Affinity Rule Generator&lt;/p&gt;
&lt;p&gt;SHOWAR的亲和性规则生成器每 5 分钟使用一次 CPU、内存和网络利用率，这是一个由 300 个数据点组成的向量（每个数据点是微服务副本的平均值）来计算每个数据点之间不同资源类型的相关系数。消除弱相关或无相关实例，[−0.8,+0.8] 中的任何值都将被丢弃。其他强负相关和强正相关微服务用于生成亲和性和反亲和性规则。资源使用模式会随着工作负载的变化（也称为工作负载转移）而变化，因此如果在随后的 5 分钟时间窗口内强烈的负相关或正相关变化超过 20%（可配置），SHOWAR 将撤销关联（或anti-affinity）规则。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SHOWAR的开销&lt;/p&gt;
&lt;p&gt;SHOWAR是作为Kubernetes的控制器构建的，它对于自动缩放器和其他类型的控制器具有高度可插拔性。SHOWAR使用常用的 Kubernetes监控代理（如Prometheus）和一个自定义eBPF指标监控代理。因此，与默认的Kubernetes自动缩放器相比，SHOWAR 不会引入任何额外的开销。此外，自动缩放器被调度在控制器节点上，并且不与调度在工作节点上的应用程序 Pod 共享资源。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
